{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HO146OTzjKE"
   },
   "source": [
    "# Word2Vec Implementation and Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGhklD5PkeOX"
   },
   "source": [
    "Overview:\n",
    "  - generate batch for skip-gram model\n",
    "  - implement two loss functions to train word embeddings\n",
    "  - tune the parameters for word embeddings\n",
    "  - apply best learned word embeddings to word analogy task\n",
    "  - calculate bias score on your best models\n",
    "  - create a new task on which you would run WEAT test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZOrRLfndVpL"
   },
   "source": [
    "How to use this notebook:\n",
    "  - This notebook is best viewed and executed in Google Colab.\n",
    "  - Please upload the .ipynb version of this notebook into Google Drive.\n",
    "  - Double click and select Open with Colab\n",
    "  - Upload the files provided in the current working directory of the Colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw0rkHABctDa"
   },
   "source": [
    "Please use the following Google Colab Tutorial in case you are not familiar with the tool: [Link](https://colab.research.google.com/drive/16pBJQePbqkz3QFV54L4NIkOn1kwpuRrj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqOwCznvRnOu"
   },
   "source": [
    "## Setting up the data and needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pAsXQn_7Rl8Q",
    "outputId": "41d312b9-b1cc-4298-ea00-5a39b89979e9"
   },
   "outputs": [],
   "source": [
    "# Download datafile for Linux\n",
    "# !wget http://mattmahoney.net/dc/text8.zip\n",
    "# !unzip text8.zip\n",
    "# !rm text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datafile for Windows\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the URL of the file\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "# Define the file name to save\n",
    "filename = \"text8.zip\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the contents of the zip file\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# Remove the zip file\n",
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aMOLGZu565p"
   },
   "source": [
    "<b>Importing needed libraries and setting up random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpjuG6Tc56H7",
    "outputId": "8c597b63-820d-47c6-c9a4-dfb4b6215f9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28d92839750>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All import statements\n",
    "\n",
    "import collections\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Setting up all the seeds for repeatable experiements\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3EdwlSd1d79"
   },
   "source": [
    "## Generating the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeRFlgId64Xw"
   },
   "source": [
    "To train word vectors, generating training instances from the given data is necessary. The generation method will create training instances in batches. For the skip-gram model, it will slide a window and sample training instances from the data inside the window.\n",
    "\n",
    "<b>For example:</b>\n",
    "\n",
    "Suppose that we have a text: \"The quick brown fox jumps over the lazy dog.\"\n",
    "and batch_size = 8, window_size = 3\n",
    "\n",
    "\"<font color = red>[The quick brown]</font> fox jumps over the lazy dog\"\n",
    "\n",
    "Context word would be 'quick' and predicting words are 'The' and 'brown'.\n",
    "\n",
    "This will generate training examples of the form context(x), predicted_word(y) like:\n",
    "<ul>\n",
    "      <li>(quick    ,       The)\n",
    "      <li>(quick    ,     brown)\n",
    "</ul>\n",
    "And then move the sliding window.\n",
    "\n",
    "\"The <font color = red>[quick brown fox]</font> jumps over the lazy dog\"\n",
    "\n",
    "In the same way, we have two more examples:\n",
    "<ul>\n",
    "    <li>(brown, quick)\n",
    "    <li>(brown, fox)\n",
    "</ul>\n",
    "\n",
    "Moving the window again:\n",
    "\n",
    "\"The quick <font color = red>[brown fox jumps]</font> over the lazy dog\"\n",
    "\n",
    "We get,\n",
    "\n",
    "<ul>\n",
    "    <li>(fox, brown)\n",
    "    <li>(fox, jumps)\n",
    "</ul>\n",
    "\n",
    "Finally we get two more instances from the moved window,\n",
    "\n",
    "\"The quick brown <font color = red>[fox jumps over]</font> the lazy dog\"\n",
    "\n",
    "<ul>\n",
    "    <li>(jumps, fox)\n",
    "    <li>(jumps, over)\n",
    "</ul>\n",
    "\n",
    "Since now we have 8 training instances, which is the batch size,\n",
    "stop generating this batch and return batch data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_HuugPXXuev"
   },
   "source": [
    "\n",
    "The two functions given below can fetch the data from the file streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5NM90locyL99"
   },
   "outputs": [],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    with open(filename) as file:\n",
    "        text = file.read()\n",
    "        data = [token.lower() for token in text.strip().split(\" \")]\n",
    "    return data\n",
    "\n",
    "def build_dataset(words, vocab_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "    # token_to_id dictionary, id_to_taken reverse_dictionary\n",
    "    vocab_token_to_id = dict()\n",
    "    for word, _ in count:\n",
    "        vocab_token_to_id[word] = len(vocab_token_to_id)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in vocab_token_to_id:\n",
    "            index = vocab_token_to_id[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    vocab_id_to_token = dict(zip(vocab_token_to_id.values(), vocab_token_to_id.keys()))\n",
    "    return data, count, vocab_token_to_id, vocab_id_to_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn-6QljIlSII"
   },
   "source": [
    "<b>Variable Description</b>\n",
    "\n",
    "data_index is the index of a word. Access a word using data[data_index].\n",
    "\n",
    "batch_size is the number of instances in one batch.\n",
    "\n",
    "num_skips is the number of samples draw in a window (in example, it was 2).\n",
    "\n",
    "skip_windows decides how many words to consider left and right from a context word(so, skip_windows*2+1 = window_size).\n",
    "\n",
    "batch will contains word ids for context words. Dimension is [batch_size].\n",
    "\n",
    "labels will contains word ids for predicting words. Dimension is [batch_size, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6AVfTjwS9Q_L"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, batch_size=128, num_skips=8, skip_window=4):\n",
    "        \"\"\"\n",
    "        @data_index: the index of a word. You can access a word using data[data_index]\n",
    "        @batch_size: the number of instances in one batch\n",
    "        @num_skips: the number of samples you want to draw in a window\n",
    "                (In the below example, it was 2)\n",
    "        @skip_window: decides how many words to consider left and right from a context word.\n",
    "                    (So, skip_windows*2+1 = window_size)\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_index=0\n",
    "        self.data = data\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "\n",
    "    def reset_index(self, idx=0):\n",
    "        self.data_index=idx\n",
    "\n",
    "    def generate_batch(self):\n",
    "        \"\"\"\n",
    "        Write the code generate a training batch\n",
    "\n",
    "        batch will contain word ids for context words. Dimension is [batch_size].\n",
    "        labels will contain word ids for predicting(target) words. Dimension is [batch_size, 1].\n",
    "        \"\"\"\n",
    "        # print(\"Generating batchs...\")\n",
    "        center_word = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        context_word = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "\n",
    "        # stride: for the rolling window\n",
    "        stride = 1\n",
    "\n",
    "        # TODO(students): (DONE)\n",
    "        # print(\"DATA: \")\n",
    "        # print(\"Shape: \", np_data.shape)\n",
    "        if (self.data_index == 0):\n",
    "            self.data_index = self.skip_window\n",
    "        else:\n",
    "            self.data_index+= self.skip_window+1\n",
    "        # print(\"data_index\")\n",
    "        # print(self.data_index)\n",
    "        # print(self.data[self.data_index])\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        for i in range(self.batch_size // self.num_skips):\n",
    "          center_word_index = self.data_index\n",
    "          selected_context_words = set()\n",
    "          for j in range(self.num_skips):\n",
    "            while True:\n",
    "              context_word_index = np.random.randint(\n",
    "                  max(0, self.data_index - self.skip_window),\n",
    "                  min(len(self.data)-1, self.data_index + self.skip_window)+1\n",
    "              )\n",
    "              if context_word_index != self.data_index and context_word_index not in selected_context_words:\n",
    "                    break\n",
    "            center_word[i * self.num_skips + j] = self.data[center_word_index]\n",
    "            context_word[i * self.num_skips + j] = self.data[context_word_index]\n",
    "            selected_context_words.add(context_word_index)\n",
    "\n",
    "          self.data_index = (self.data_index + stride) % len(self.data)\n",
    "        # print(\"CONTEXT: \")\n",
    "        # print(context_word.shape)\n",
    "        # print(\"CENTER: \")\n",
    "        # print(center_word.shape)\n",
    "\n",
    "\n",
    "        return torch.LongTensor(center_word), torch.LongTensor(context_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_Rl4BwSNibp"
   },
   "source": [
    "## Building the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neDNj2jlmTG0"
   },
   "source": [
    "\n",
    "<b>Negative Log Likelihood (NLL): </b>\n",
    "A metric used in statistics and machine learning to evaluate how well a model fits the observed data. It measures the dissimilarity between the predicted probability distribution and the actual distribution of the data. By taking the negative logarithm of the likelihood function, NLL converts the task of maximizing likelihood into minimizing a loss, making it suitable for optimization algorithms. Lower values of NLL indicate better agreement between the model's predictions and the actual data, making it a commonly used measure in tasks like classification and regression.\n",
    "\n",
    "Refer to [here](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).\n",
    "\n",
    "Training a word2vec model with this loss and the default settings took ~50 mins on Google Colab with GPU accelarator. It will take ~10 hrs on a Macbook Pro 2018 CPU.\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Negative Sampling (NEG): </b>\n",
    "The negative sampling formulates a slightly different classification task and a corresponding loss.\n",
    "[This paper](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) describes the method in detail.\n",
    "\n",
    "The idea here is to build a classifier that can give high probabilities to words that are the correct target words and low probabilities to words that are incorrect target words.\n",
    "As with negative log likelihood loss, here we define the classifier using a function that uses the word vectors of the context and target as free parameters.\n",
    "The key difference however is that instead of using the entire vocabulary, here we sample a set of k negative words for each instance, and create an augmented instance which is a collection of the true target word and k negative words.\n",
    "Now the vectors are trained to maximize the probability of this augmented instance.\n",
    "To understand it better, you may also refer to [here](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).\n",
    "\n",
    "Training a word2vec model with this loss and the default settings took ~2h30 mins on Google Colab with GPU accelarator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FFsQcYx2MJfj"
   },
   "outputs": [],
   "source": [
    "# Defining the sigmoid function\n",
    "sigmoid = lambda x: 1/(1 + torch.exp(-x))\n",
    "\n",
    "class WordVec(nn.Module):\n",
    "    def __init__(self, V, embedding_dim, loss_func, counts, num_neg_samples_per_center = 1):\n",
    "        super(WordVec, self).__init__()\n",
    "        self.center_embeddings = nn.Embedding(num_embeddings=V, embedding_dim=embedding_dim)\n",
    "        self.center_embeddings.weight.data.normal_(mean=0, std=1/math.sqrt(embedding_dim))\n",
    "        self.center_embeddings.weight.data[self.center_embeddings.weight.data<-1] = -1\n",
    "        self.center_embeddings.weight.data[self.center_embeddings.weight.data>1] = 1\n",
    "\n",
    "        self.context_embeddings = nn.Embedding(num_embeddings=V, embedding_dim=embedding_dim)\n",
    "        self.context_embeddings.weight.data.normal_(mean=0, std=1/math.sqrt(embedding_dim))\n",
    "        self.context_embeddings.weight.data[self.context_embeddings.weight.data<-1] = -1 + 1e-10\n",
    "        self.context_embeddings.weight.data[self.context_embeddings.weight.data>1] = 1 - 1e-10\n",
    "\n",
    "        self.loss_func = loss_func\n",
    "        self.counts = counts\n",
    "\n",
    "        self.num_neg_samples_per_center = num_neg_samples_per_center\n",
    "\n",
    "    def forward(self, center_word, context_word):\n",
    "\n",
    "        if self.loss_func == \"nll\":\n",
    "            return self.negative_log_likelihood_loss(center_word, context_word)\n",
    "        elif self.loss_func == \"neg\":\n",
    "            return self.negative_sampling(center_word, context_word)\n",
    "        else:\n",
    "            raise Exception(\"No implementation found for %s\"%(self.loss_func))\n",
    "\n",
    "    def negative_log_likelihood_loss(self, center_word, context_word):\n",
    "\n",
    "        # Notes (page 9): http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf\n",
    "        center_word_embeddings = self.center_embeddings(center_word) # batches, dims\n",
    "        context_word_embeddings = self.context_embeddings(context_word) # batches, dims\n",
    "\n",
    "        a = torch.sum(torch.mul(center_word_embeddings, context_word_embeddings), axis=1) # batches\n",
    "        # (batches, dims) @ (dims, V) = (batches, V);\n",
    "        b = torch.logsumexp(center_word_embeddings @ self.context_embeddings.weight.T, dim=1) # batches\n",
    "        loss = torch.mean(b - a)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def negative_sampling(self, center_word, context_word):\n",
    "\n",
    "        # use this variable to control the number of negative samples for every positive sample\n",
    "        # print(center_word[:20])\n",
    "        # print(context_word[:20])\n",
    "        num_neg_samples_per_center = self.num_neg_samples_per_center\n",
    "\n",
    "\n",
    "        # TODO(students) (done):\n",
    "        batch_size = center_word.shape[0]\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # create a tensor with the negative samples\n",
    "        neg_samples = []\n",
    "        for i in range(batch_size):\n",
    "            neg_samples.append(torch.multinomial(torch.tensor(self.counts).float().pow(0.75), self.num_neg_samples_per_center, replacement=True))\n",
    "        neg_samples = torch.stack(neg_samples).to(device)  # (batch_size, num_neg_samples_per_center)\n",
    "\n",
    "        center_word_embeddings = self.center_embeddings(center_word).to(device)  # (batch_size, embedding_dim)\n",
    "        context_word_embeddings = self.context_embeddings(context_word).to(device)  # (batch_size, embedding_dim)\n",
    "\n",
    "        neg_word_embeddings = self.context_embeddings(neg_samples).to(device)  # (batch_size, num_neg_samples_per_center, embedding_dim)\n",
    "\n",
    "        # compute the dot product of the center and context word embeddings\n",
    "        pos_scores = torch.sum(torch.mul(center_word_embeddings, context_word_embeddings), dim=1)  # (batch_size,)\n",
    "\n",
    "        # compute the dot product of the center word embeddings and the negative samples embeddings\n",
    "        neg_scores = torch.bmm(neg_word_embeddings, center_word_embeddings.unsqueeze(2)).squeeze()  # (batch_size, num_neg_samples_per_center)\n",
    "\n",
    "        # compute the loss\n",
    "        pos_loss = torch.mean(-torch.log(sigmoid(pos_scores)))\n",
    "        neg_loss = torch.mean(-torch.log(sigmoid(-neg_scores)))\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def print_closest(self, validation_words, reverse_dictionary, top_k=8):\n",
    "        print('Printing closest words')\n",
    "        embeddings = torch.zeros(self.center_embeddings.weight.shape).copy_(self.center_embeddings.weight)\n",
    "        embeddings = embeddings.data.cpu().numpy()\n",
    "\n",
    "        validation_ids = validation_words\n",
    "        norm = np.sqrt(np.sum(np.square(embeddings),axis=1,keepdims=True))\n",
    "        normalized_embeddings = embeddings/norm\n",
    "        validation_embeddings = normalized_embeddings[validation_ids]\n",
    "        similarity = np.matmul(validation_embeddings, normalized_embeddings.T)\n",
    "        for i in range(len(validation_ids)):\n",
    "            word = reverse_dictionary[validation_words[i]]\n",
    "            nearest = (-similarity[i, :]).argsort()[1:top_k+1]\n",
    "            print(word, [reverse_dictionary[nearest[k]] for k in range(top_k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJ6wwwGdPk90"
   },
   "source": [
    "## Training and Data Loading Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw3TQl17Zrzr"
   },
   "source": [
    "The code below uses the models and losses built above and runs the actual training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6_qwLRzKPdiw"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, ckpt_save_path, reverse_dictionary):\n",
    "        self.model = model\n",
    "        self.ckpt_save_path = ckpt_save_path\n",
    "        self.reverse_dictionary = reverse_dictionary\n",
    "\n",
    "    def training_step(self, center_word, context_word):\n",
    "        loss =  self.model(center_word, context_word)\n",
    "        return loss\n",
    "\n",
    "    def train(self, dataset, max_training_steps, ckpt_steps, validation_words, device=\"cpu\", lr = 1):\n",
    "\n",
    "        optim = torch.optim.SGD(self.model.parameters(), lr = lr)\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        self.losses = []\n",
    "\n",
    "        t = tqdm(range(max_training_steps))\n",
    "        for curr_step in t:\n",
    "            optim.zero_grad()\n",
    "            center_word, context_word = dataset.generate_batch()\n",
    "            loss = self.training_step(center_word.to(device), context_word.to(device))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            self.losses.append(loss.item())\n",
    "            if curr_step:\n",
    "                t.set_description(\"Avg loss: %s\"%(round(sum(self.losses[-2000:])/len(self.losses[-2000:]), 3)))\n",
    "            # if curr_step % 10000 == 0:\n",
    "            #     self.model.print_closest(validation_words, self.reverse_dictionary)\n",
    "            if curr_step%ckpt_steps == 0 and curr_step > 0:\n",
    "                self.save_ckpt(curr_step)\n",
    "\n",
    "    def save_ckpt(self, curr_step):\n",
    "        torch.save(self.model, \"%s/%s.pt\"%(self.ckpt_save_path, str(curr_step)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys10LdRRQp1Z"
   },
   "source": [
    "## Training Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmU-8MSepfLf"
   },
   "source": [
    "The following run_training function will train a model as shown in the results below. The parameters of the run_training() function include many hyperparameters which can be experimented with. Some examples include vector size, batch size, vocabulary size, epochs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_EeC-lidQspm"
   },
   "outputs": [],
   "source": [
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print (\"Created a path: %s\"%(path))\n",
    "\n",
    "def run_training(\n",
    "    model_type = 'nll', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 1, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 1, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    "):\n",
    "\n",
    "    checkpoint_model_path = f'{checkpoint_model_path}_{model_type}/'\n",
    "    create_path(checkpoint_model_path)\n",
    "\n",
    "    # Read data\n",
    "    words = read_data(\"./text8\")\n",
    "    print('Data size', len(words))\n",
    "\n",
    "    data, count, vocab_token_to_id, vocab_id_to_token = build_dataset(words, vocab_size)\n",
    "    # save dictionary as vocabulary\n",
    "    print('Most common words (+UNK)', count[:5])\n",
    "    print('Sample data', data[:10], [vocab_id_to_token[i] for i in data[:10]])\n",
    "    # Calculate the probability of unigrams\n",
    "    # unigram_cnt = [c for w, c in count]\n",
    "    count_dict = dict(count)\n",
    "    unigram_cnt = [count_dict[vocab_id_to_token[i]] for i in sorted(list(vocab_token_to_id.values()))]\n",
    "    data_index = 0\n",
    "\n",
    "    dataset = Dataset(data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window)\n",
    "    center, context = dataset.generate_batch()\n",
    "    for i in range(8):\n",
    "        print(center[i].item(), vocab_id_to_token[center[i].item()],'->', context[i].item(), vocab_id_to_token[context[i].item()])\n",
    "    dataset.reset_index()\n",
    "\n",
    "    valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "    embedding_size = embedding_size\n",
    "    model = WordVec(V=vocab_size, embedding_dim=embedding_size, loss_func=model_type, counts=np.array(unigram_cnt), num_neg_samples_per_center = num_neg_samples_per_center)\n",
    "    trainer = Trainer(model, checkpoint_model_path, vocab_id_to_token)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    trainer.train(dataset, max_num_steps, checkpoint_step, valid_examples, device, lr = lr)\n",
    "    model_path = final_model_path\n",
    "    create_path(model_path)\n",
    "    model_filepath = os.path.join(model_path, 'word2vec_%s.model'%(model_type))\n",
    "    pickle.dump([vocab_token_to_id, model.center_embeddings.weight.detach().cpu().numpy()], open(model_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg9Hobhyqja_"
   },
   "source": [
    "The following cell shows a demo with much lesser training epochs, embedding size and vocabulary size to test the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCOcE0Uqej4t",
    "outputId": "cb6528e6-3851-4af0-c461-586cc0f6a5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 189230], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3081 originated -> 12 as\n",
      "3081 originated -> 5234 anarchism\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.341: 100%|█████████████████████████████████████████████████████████████| 2001/2001 [06:48<00:00,  4.90it/s]\n"
     ]
    }
   ],
   "source": [
    "run_training(\n",
    "    model_type = 'neg', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 10, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 3, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './demo_checkpoints', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_demo_model', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 256, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 4, # size of the embedding vectores\n",
    "    checkpoint_step = 500, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 2001 # Maximum number of steps to train for\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLY01IbAeww5"
   },
   "source": [
    "<b>Train models of NLL and NEG using the above function.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXSBmxqpEVcr",
    "outputId": "fde1b4c3-f8ac-4a67-843b-353c0ce8beda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a path: ./checkpoints_neg/\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 189230], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3081 originated -> 5234 anarchism\n",
      "3081 originated -> 12 as\n",
      "12 as -> 6 a\n",
      "12 as -> 3081 originated\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n",
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/200001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['barajas', 'meter', 'nonsense', 'environmental', 'sharpest', 'kincaid', 'halloween', 'micromanagement']\n",
      "th ['deaden', 'holmia', 'ddp', 'antonello', 'dostum', 'fortuyn', 'bebop', 'stepchildren']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 1/200001 [00:00<14:57:03,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ['hydrocarbons', 'daemons', 'ruptures', 'mutinied', 'locations', 'petra', 'gascons', 'sanj']\n",
      "about ['quanto', 'sentience', 'voice', 'greer', 'yrigoyen', 'murabit', 'buchner', 'nickels']\n",
      "as ['liddle', 'cellphones', 'webserver', 'mannered', 'moca', 'algebraists', 'smythe', 'fynbos']\n",
      "no ['sanshoku', 'skirting', 'poaching', 'monogatari', 'zaki', 'cadr', 'sandstorms', 'scents']\n",
      "to ['priscillian', 'pamphylia', 'harland', 'santer', 'subscribed', 'canceled', 'rasul', 'valsartan']\n",
      "been ['huis', 'transferrin', 'market', 'chlein', 'additions', 'ravenscroft', 'popoff', 'nahash']\n",
      "may ['variable', 'inebriation', 'adventurers', 'denouement', 'accrington', 'pella', 'unreason', 'watusi']\n",
      "six ['esm', 'deferment', 'gameday', 'nucellus', 'rebalancing', 'ylf', 'zeevi', 'fudge']\n",
      "UNK ['hardiness', 'simulant', 'commonly', 'khalqi', 'elisp', 'adgb', 'corrupting', 'bessie']\n",
      "seven ['etxebarrieta', 'bioerosion', 'ignimbrite', 'vetter', 'aiga', 'shelly', 'bracks', 'kango']\n",
      "while ['heredity', 'meiningen', 'nvg', 'nodule', 'jasta', 'dreamweaver', 'eiger', 'scripts']\n",
      "for ['tartakower', 'choke', 'asado', 'dealerships', 'thang', 'propagandists', 'marley', 'belisario']\n",
      "s ['hats', 'genitals', 'rhymer', 'tes', 'rms', 'culshaw', 'grayish', 'lecture']\n",
      "used ['succulent', 'batthy', 'oppositions', 'proportionate', 'ipv', 'autocrat', 'hahn', 'depresses']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.255:   5%|██▊                                                     | 10001/200001 [20:16<8:28:35,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['during', 'end', 'same', 'mouth', 'laws', 'removal', 'university', 'siona']\n",
      "th ['six', 'four', 'seven', 'three', 'two', 'nine', 'eight', 'five']\n",
      "all ['locations', 'fact', 'unexplored', 'cellar', 'angola', 'cacao', 'embassies', 'malwa']\n",
      "about ['four', 'eight', 'three', 'six', 'seven', 'km', 'five', 'two']\n",
      "as ['monitoring', 'few', 'batcave', 'by', 'borne', 'backspace', 'confoederatio', 'within']\n",
      "no ['it', 'he', 'she', 'cmj', 'raeder', 'this', 'generalship', 'there']\n",
      "to ['can', 'with', 'may', 'for', 'dumplings', 'in', 'onan', 'dddddd']\n",
      "been ['may', 'bulging', 'merchandising', 'seria', 'guaymas', 'huis', 'harbor', 'market']\n",
      "may ['can', 'would', 'could', 'will', 'been', 'should', 'to', 'legalise']\n",
      "six ['four', 'five', 'seven', 'eight', 'three', 'two', 'nine', 'one']\n",
      "UNK ['agave', 'intriguing', 'order', 'unachievable', 'mi', 'heidi', 'muggs', 'guangzhou']\n",
      "seven ['nine', 'eight', 'six', 'four', 'three', 'five', 'two', 'isbn']\n",
      "while ['scripts', 'creating', 'solidifies', 'cantacuzenus', 'eiger', 'nodule', 'kauffman', 'longest']\n",
      "for ['of', 'from', 'with', 'in', 'on', 'by', 'at', 'and']\n",
      "s ['in', 'of', 'the', 'or', 'rabaul', 'with', 'ziyi', 'and']\n",
      "used ['known', 'many', 'an', 'located', 'well', 'more', 'considered', 'found']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.219:  10%|█████▌                                                  | 20001/200001 [40:44<8:09:56,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['during', 'same', 'blackjacks', 'including', 'world', 'after', 'under', 'own']\n",
      "th ['four', 'three', 'seven', 'bc', 'six', 'eight', 'five', 'two']\n",
      "all ['fact', 'malwa', 'tablets', 'unexplored', 'armistice', 'cacao', 'area', 'locations']\n",
      "about ['b', 'n', 'january', 'over', 'four', 'seven', 'eight', 'x']\n",
      "as ['monitoring', 'confoederatio', 'within', 'for', 'with', 'were', 'pelly', 'by']\n",
      "no ['she', 'it', 'he', 'there', 'they', 'still', 'often', 'only']\n",
      "to ['may', 'exhortations', 'onan', 'with', 'hailing', 'should', 'clasped', 'pasts']\n",
      "been ['never', 'become', 'could', 'not', 'you', 'had', 'may', 'bulging']\n",
      "may ['can', 'could', 'would', 'should', 'will', 'must', 'to', 'been']\n",
      "six ['four', 'eight', 'seven', 'five', 'nine', 'three', 'two', 'zero']\n",
      "UNK ['l', 'adgb', 'r', 'desegregate', 'mi', 'size', 'intriguing', 'cream']\n",
      "seven ['eight', 'four', 'six', 'three', 'nine', 'five', 'two', 'zero']\n",
      "while ['though', 'since', 'creating', 'during', 'scripts', 'are', 'were', 'however']\n",
      "for ['from', 'at', 'in', 'on', 'after', 'with', 'by', 'of']\n",
      "s ['his', 'the', 'in', 'of', 'tavy', 'and', 'senior', 'rabaul']\n",
      "used ['known', 'considered', 'found', 'described', 'seen', 'given', 'possible', 'led']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.174:  15%|████████                                              | 30001/200001 [1:01:15<7:37:09,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['same', 'during', 'last', 'second', 'nias', 'blackjacks', 'city', 'herlihy']\n",
      "th ['bc', 'four', 'seven', 'three', 'six', 'five', 'eight', 'two']\n",
      "all ['fact', 'these', 'some', 'results', 'species', 'malwa', 'terrace', 'sven']\n",
      "about ['five', 'eight', 'four', 'six', 'seven', 'three', 'one', 'two']\n",
      "as ['is', 'after', 'into', 'for', 'by', 'was', 'in', 'monitoring']\n",
      "no ['she', 'they', 'it', 'still', 'we', 'now', 'often', 'who']\n",
      "to ['may', 'can', 'with', 'into', 'hailing', 'should', 'pasts', 'not']\n",
      "been ['never', 'become', 'be', 'had', 'could', 'harvey', 'bulging', 'we']\n",
      "may ['can', 'could', 'would', 'should', 'must', 'will', 'to', 'might']\n",
      "six ['four', 'eight', 'seven', 'five', 'nine', 'zero', 'three', 'two']\n",
      "UNK ['l', 'zero', 'six', 'one', 'eight', 'seven', 'r', 'four']\n",
      "seven ['eight', 'four', 'nine', 'six', 'five', 'zero', 'three', 'two']\n",
      "while ['though', 'where', 'when', 'during', 'were', 'however', 'throughout', 'since']\n",
      "for ['from', 'at', 'on', 'after', 'during', 'within', 'against', 'with']\n",
      "s ['his', 'of', 'and', 'the', 'at', 'after', 'in', 'see']\n",
      "used ['described', 'known', 'considered', 'found', 'given', 'seen', 'regarded', 'written']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.169:  20%|██████████▊                                           | 40001/200001 [1:21:49<7:48:16,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['same', 'last', 'during', 'second', 'son', 'next', 'following', 'city']\n",
      "th ['bc', 'seven', 'three', 'four', 'five', 'six', 'eight', 'nine']\n",
      "all ['these', 'some', 'several', 'sven', 'forms', 'species', 'many', 'conjugation']\n",
      "about ['four', 'six', 'five', 'eight', 'three', 'two', 'seven', 'september']\n",
      "as ['when', 'by', 'after', 'was', 'are', 'for', 'is', 'were']\n",
      "no ['she', 'they', 'only', 'it', 'he', 'we', 'there', 'a']\n",
      "to ['not', 'may', 'can', 'into', 'hailing', 'with', 'should', 'been']\n",
      "been ['be', 'had', 'never', 'was', 'become', 'not', 'harvey', 'could']\n",
      "may ['can', 'would', 'could', 'should', 'will', 'must', 'to', 'might']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'r', 'n', 'six', 'four', 'eight', 'zero', 'e']\n",
      "seven ['eight', 'six', 'four', 'five', 'nine', 'zero', 'three', 'two']\n",
      "while ['though', 'however', 'when', 'where', 'although', 'were', 'gave', 'became']\n",
      "for ['at', 'from', 'against', 'during', 'after', 'in', 'under', 'including']\n",
      "s ['his', 'of', 'andrew', 'the', 'its', 'second', 'b', 'senior']\n",
      "used ['found', 'described', 'known', 'considered', 'located', 'written', 'put', 'actually']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.155:  25%|█████████████▍                                        | 50000/200001 [1:42:26<5:00:04,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'same', 'during', 'next', 'son', 'following', 'death']\n",
      "th ['bc', 'seven', 'six', 'st', 'five', 'eight', 'four', 'three']\n",
      "all ['some', 'these', 'time', 'forms', 'area', 'several', 'age', 'sven']\n",
      "about ['over', 'eight', 'seven', 'four', 'september', 'two', 'five', 'six']\n",
      "as ['when', 'nine', 'is', 'was', 'by', 'are', 'after', 'were']\n",
      "no ['she', 'only', 'they', 'generally', 'there', 'quite', 'a', 'another']\n",
      "to ['not', 'may', 'can', 'would', 'into', 'could', 'hailing', 'been']\n",
      "been ['be', 'had', 'was', 'become', 'not', 'have', 'never', 'also']\n",
      "may ['can', 'would', 'could', 'should', 'will', 'must', 'is', 'to']\n",
      "six ['five', 'four', 'seven', 'eight', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'six', 'n', 'four', 'seven', 'five', 'eight', 'nine']\n",
      "seven ['eight', 'four', 'six', 'five', 'nine', 'zero', 'three', 'two']\n",
      "while ['where', 'when', 'though', 'however', 'although', 'gave', 'became', 'were']\n",
      "for ['at', 'against', 'from', 'between', 'including', 'within', 'during', 'under']\n",
      "s ['his', 'the', 'andrew', 'iii', 'war', 'its', 'b', 'yngwie']\n",
      "used ['found', 'known', 'described', 'located', 'written', 'considered', 'put', 'well']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.164:  30%|████████████████▏                                     | 60001/200001 [2:03:03<6:18:53,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['same', 'last', 'second', 'during', 'next', 'son', 'end', 'following']\n",
      "th ['bc', 'six', 'seven', 'five', 'eight', 'st', 'nine', 'three']\n",
      "all ['these', 'some', 'over', 'three', 'five', 'time', 'four', 'them']\n",
      "about ['over', 'atari', 'four', 'five', 'around', 'eight', 'nine', 'six']\n",
      "as ['when', 'became', 'within', 'be', 'before', 'after', 'is', 'where']\n",
      "no ['only', 'any', 'she', 'they', 'there', 'a', 'another', 'it']\n",
      "to ['can', 'may', 'not', 'would', 'into', 'over', 'only', 'could']\n",
      "been ['be', 'had', 'was', 'become', 'not', 'also', 'have', 'often']\n",
      "may ['can', 'would', 'could', 'will', 'should', 'must', 'to', 'is']\n",
      "six ['five', 'seven', 'four', 'eight', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'r', 'n', 'm', 'six', 'four', 'three', 'eight']\n",
      "seven ['six', 'eight', 'four', 'five', 'nine', 'zero', 'three', 'two']\n",
      "while ['however', 'where', 'although', 'though', 'when', 'but', 'gave', 'until']\n",
      "for ['at', 'against', 'under', 'between', 'within', 'without', 'when', 'above']\n",
      "s ['his', 'of', 'its', 'second', 'new', 'b', 'old', 'british']\n",
      "used ['known', 'found', 'called', 'well', 'nine', 'seven', 'described', 'eight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.161:  35%|██████████████████▉                                   | 70001/200001 [2:23:43<6:08:37,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['same', 'last', 'next', 'second', 'during', 'end', 'old', 'following']\n",
      "th ['six', 'seven', 'eight', 'nine', 'three', 'five', 'bc', 'zero']\n",
      "all ['these', 'some', 'both', 'many', 'use', 'several', 'other', 'them']\n",
      "about ['over', 'eight', 'atari', 'six', 'five', 'four', 'around', 'seven']\n",
      "as ['when', 'within', 'became', 'after', 'while', 'nine', 'be', 'zero']\n",
      "no ['only', 'any', 'she', 'another', 'it', 'there', 'some', 'approximately']\n",
      "to ['can', 'may', 'not', 'would', 'only', 'will', 'could', 'should']\n",
      "been ['be', 'had', 'not', 'was', 'have', 'him', 'were', 'become']\n",
      "may ['can', 'would', 'could', 'will', 'should', 'must', 'to', 'found']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'r', 'seven', 'six', 'eight', 'four', 'n', 'm']\n",
      "seven ['six', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "while ['although', 'when', 'though', 'however', 'where', 'but', 'gave', 'during']\n",
      "for ['at', 'against', 'within', 'without', 'when', 'above', 'after', 'under']\n",
      "s ['his', 'her', 'andrew', 'iii', 'second', 'b', 'new', 'rzeczypospolitej']\n",
      "used ['known', 'found', 'nine', 'eight', 'called', 'seven', 'six', 'well']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.152:  40%|█████████████████████▌                                | 80001/200001 [2:44:24<5:26:32,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'same', 'during', 'end', 'name', 'one']\n",
      "th ['six', 'seven', 'eight', 'nine', 'five', 'zero', 'three', 'four']\n",
      "all ['some', 'both', 'these', 'over', 'three', 'two', 'four', 'six']\n",
      "about ['over', 'around', 'on', 'age', 'nine', 'under', 'seven', 'that']\n",
      "as ['became', 'when', 'be', 'while', 'within', 'first', 'after', 'nine']\n",
      "no ['only', 'any', 'she', 'there', 'another', 'it', 'they', 'some']\n",
      "to ['can', 'may', 'would', 'not', 'will', 'must', 'could', 'into']\n",
      "been ['be', 'had', 'was', 'become', 'not', 'have', 'him', 'were']\n",
      "may ['can', 'will', 'could', 'would', 'must', 'should', 'found', 'to']\n",
      "six ['seven', 'five', 'eight', 'four', 'nine', 'zero', 'three', 'two']\n",
      "UNK ['l', 'r', 'v', 'g', 'b', 'e', 'seven', 'n']\n",
      "seven ['six', 'eight', 'nine', 'five', 'four', 'zero', 'three', 'two']\n",
      "while ['although', 'when', 'though', 'however', 'where', 'but', 'during', 'before']\n",
      "for ['when', 'within', 'against', 'through', 'above', 'without', 'while', 'if']\n",
      "s ['his', 'british', 'second', 'her', 'b', 'largest', 'new', 'city']\n",
      "used ['known', 'found', 'eight', 'seven', 'nine', 'well', 'called', 'zero']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.141:  45%|████████████████████████▎                             | 90001/200001 [3:05:08<4:59:34,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'same', 'book', 'one', 'end', 'during']\n",
      "th ['six', 'seven', 'nine', 'eight', 'five', 'four', 'zero', 'three']\n",
      "all ['both', 'some', 'two', 'many', 'these', 'three', 'other', 'over']\n",
      "about ['over', 'around', 'age', 'only', 'seven', 'atari', 'all', 'on']\n",
      "as ['became', 'while', 'when', 'within', 'although', 'was', 'before', 'because']\n",
      "no ['only', 'any', 'another', 'a', 'it', 'there', 'they', 'so']\n",
      "to ['not', 'will', 'can', 'would', 'may', 'could', 'must', 'through']\n",
      "been ['be', 'had', 'become', 'have', 'often', 'not', 'was', 'were']\n",
      "may ['can', 'will', 'would', 'could', 'must', 'should', 'found', 'might']\n",
      "six ['seven', 'five', 'eight', 'four', 'nine', 'zero', 'three', 'two']\n",
      "UNK ['l', 'g', 'r', 'n', 'e', 'p', 'm', 'h']\n",
      "seven ['six', 'eight', 'five', 'nine', 'four', 'zero', 'three', 'two']\n",
      "while ['although', 'when', 'however', 'though', 'before', 'where', 'during', 'but']\n",
      "for ['within', 'against', 'when', 'if', 'without', 'through', 'while', 'although']\n",
      "s ['his', 'third', 'second', 'series', 'rzeczypospolitej', 'new', 'her', 'british']\n",
      "used ['found', 'known', 'eight', 'nine', 'seven', 'six', 'well', 'called']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.123:  50%|██████████████████████████▍                          | 100000/200001 [3:25:53<3:21:48,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'same', 'third', 'left', 'old', 'book']\n",
      "th ['six', 'five', 'seven', 'nine', 'eight', 'zero', 'four', 'one']\n",
      "all ['both', 'some', 'these', 'them', 'many', 'three', 'each', 'over']\n",
      "about ['over', 'around', 'only', 'three', 'given', 'all', 'eight', 'two']\n",
      "as ['when', 'while', 'became', 'within', 'because', 'although', 'so', 'before']\n",
      "no ['only', 'any', 'another', 'very', 'a', 'it', 'they', 'she']\n",
      "to ['not', 'will', 'may', 'can', 'would', 'could', 'out', 'into']\n",
      "been ['be', 'had', 'become', 'have', 'was', 'often', 'were', 'not']\n",
      "may ['can', 'will', 'would', 'could', 'must', 'should', 'might', 'cannot']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'n', 'r', 'p', 'g', 'v', 'm', 'e']\n",
      "seven ['eight', 'six', 'five', 'four', 'nine', 'zero', 'three', 'two']\n",
      "while ['although', 'when', 'though', 'however', 'during', 'where', 'before', 'but']\n",
      "for ['when', 'against', 'if', 'while', 'without', 'through', 'although', 'following']\n",
      "s ['his', 'third', 'second', 'her', 'big', 'its', 'series', 'new']\n",
      "used ['written', 'found', 'known', 'nine', 'seven', 'eight', 'called', 'zero']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.127:  55%|█████████████████████████████▏                       | 110001/200001 [3:47:16<5:00:18,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'same', 'book', 'during', 'following']\n",
      "th ['six', 'five', 'seven', 'eight', 'nine', 'four', 'zero', 'bc']\n",
      "all ['both', 'some', 'these', 'about', 'many', 'them', 'each', 'two']\n",
      "about ['over', 'around', 'all', 'only', 'four', 'two', 'one', 'last']\n",
      "as ['became', 'when', 'within', 'although', 'while', 'because', 'today', 'before']\n",
      "no ['only', 'any', 'another', 'very', 'some', 'a', 'so', 'there']\n",
      "to ['not', 'would', 'will', 'could', 'can', 'may', 'should', 'through']\n",
      "been ['be', 'had', 'become', 'often', 'have', 'were', 'was', 'not']\n",
      "may ['can', 'would', 'could', 'will', 'must', 'should', 'might', 'cannot']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'm', 'charles', 'r', 'g', 'n', 'v', 'p']\n",
      "seven ['six', 'eight', 'five', 'four', 'nine', 'zero', 'three', 'two']\n",
      "while ['although', 'when', 'though', 'during', 'however', 'before', 'including', 'gave']\n",
      "for ['when', 'if', 'against', 'while', 'without', 'above', 'within', 'through']\n",
      "s ['his', 'her', 'whose', 'second', 'big', 'bob', 'third', 'british']\n",
      "used ['found', 'known', 'written', 'nine', 'considered', 'called', 'seven', 'made']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.119:  60%|███████████████████████████████▊                     | 120001/200001 [4:08:27<3:37:41,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'same', 'book', 'old', 'following']\n",
      "th ['six', 'nine', 'seven', 'eight', 'five', 'zero', 'four', 'bc']\n",
      "all ['both', 'some', 'these', 'many', 'several', 'any', 'other', 'each']\n",
      "about ['over', 'around', 'given', 'only', 'no', 'last', 'made', 'next']\n",
      "as ['because', 'within', 'when', 'became', 'although', 'today', 'while', 'so']\n",
      "no ['any', 'only', 'another', 'about', 'games', 'a', 'every', 'some']\n",
      "to ['would', 'will', 'may', 'can', 'not', 'could', 'them', 'should']\n",
      "been ['be', 'had', 'become', 'well', 'not', 'have', 'often', 'him']\n",
      "may ['can', 'will', 'would', 'could', 'must', 'should', 'might', 'cannot']\n",
      "six ['eight', 'seven', 'five', 'four', 'nine', 'zero', 'three', 'two']\n",
      "UNK ['l', 'g', 'r', 'charles', 'm', 'la', 'v', 'e']\n",
      "seven ['six', 'eight', 'five', 'nine', 'four', 'zero', 'three', 'two']\n",
      "while ['although', 'when', 'though', 'before', 'during', 'including', 'however', 'but']\n",
      "for ['when', 'without', 'above', 'against', 'if', 'although', 'using', 'common']\n",
      "s ['third', 'second', 'his', 'largest', 'rzeczypospolitej', 'her', 'national', 'british']\n",
      "used ['found', 'known', 'written', 'considered', 'nine', 'seen', 'played', 'seven']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.107:  65%|██████████████████████████████████▍                  | 130001/200001 [4:29:26<3:12:30,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'same', 'following', 'during', 'book']\n",
      "th ['nine', 'six', 'seven', 'eight', 'five', 'zero', 'four', 'bc']\n",
      "all ['both', 'some', 'many', 'these', 'several', 'each', 'them', 'only']\n",
      "about ['around', 'over', 'given', 'only', 'four', 'made', 'least', 'approximately']\n",
      "as ['when', 'became', 'within', 'because', 'although', 'using', 'while', 'before']\n",
      "no ['any', 'another', 'only', 'than', 'a', 'every', 'about', 'some']\n",
      "to ['would', 'will', 'may', 'can', 'not', 'into', 'against', 'could']\n",
      "been ['be', 'become', 'had', 'well', 'not', 'often', 'him', 'being']\n",
      "may ['can', 'would', 'will', 'could', 'must', 'should', 'might', 'cannot']\n",
      "six ['seven', 'five', 'eight', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'charles', 'v', 'r', 'g', 'c', 'la', 'm']\n",
      "seven ['six', 'eight', 'five', 'four', 'nine', 'zero', 'three', 'two']\n",
      "while ['although', 'though', 'when', 'during', 'before', 'including', 'however', 'but']\n",
      "for ['when', 'against', 'while', 'following', 'although', 'if', 'common', 'without']\n",
      "s ['third', 'second', 'largest', 'his', 'charles', 'iii', 'series', 'rzeczypospolitej']\n",
      "used ['found', 'known', 'written', 'seen', 'considered', 'played', 'added', 'nine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.105:  70%|█████████████████████████████████████                | 140001/200001 [4:50:34<2:58:22,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'same', 'only', 'book', 'following']\n",
      "th ['six', 'nine', 'eight', 'zero', 'seven', 'four', 'five', 'bc']\n",
      "all ['some', 'these', 'many', 'both', 'several', 'each', 'any', 'other']\n",
      "about ['over', 'around', 'given', 'only', 'on', 'last', 'that', 'possible']\n",
      "as ['when', 'because', 'within', 'using', 'particular', 'before', 'while', 'addition']\n",
      "no ['any', 'another', 'only', 'even', 'much', 'very', 'than', 'every']\n",
      "to ['would', 'will', 'can', 'could', 'not', 'may', 'must', 'them']\n",
      "been ['be', 'become', 'had', 'often', 'well', 'being', 'not', 'usually']\n",
      "may ['can', 'would', 'will', 'could', 'must', 'should', 'might', 'cannot']\n",
      "six ['eight', 'seven', 'four', 'five', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'r', 'g', 'n', 'e', 'c', 'island', 'charles']\n",
      "seven ['six', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "while ['although', 'though', 'when', 'before', 'during', 'including', 'began', 'within']\n",
      "for ['against', 'when', 'without', 'if', 'while', 'using', 'although', 'common']\n",
      "s ['third', 'his', 'second', 'charles', 'her', 'isbn', 'bob', 'big']\n",
      "used ['found', 'seen', 'considered', 'known', 'written', 'played', 'referred', 'described']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.098:  75%|███████████████████████████████████████▋             | 150000/200001 [5:11:37<1:44:34,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'third', 'next', 'same', 'only', 'book', 'study']\n",
      "th ['nine', 'six', 'seven', 'eight', 'zero', 'five', 'four', 'bc']\n",
      "all ['both', 'these', 'many', 'some', 'each', 'any', 'every', 'several']\n",
      "about ['around', 'over', 'given', 'only', 'last', 'making', 'that', 'possible']\n",
      "as ['because', 'when', 'within', 'before', 'how', 'became', 'due', 'while']\n",
      "no ['any', 'another', 'only', 'generally', 'much', 'even', 'some', 'every']\n",
      "to ['would', 'will', 'may', 'them', 'can', 'could', 'against', 'him']\n",
      "been ['be', 'become', 'had', 'well', 'often', 'usually', 'never', 'them']\n",
      "may ['can', 'could', 'will', 'must', 'would', 'should', 'might', 'cannot']\n",
      "six ['seven', 'four', 'five', 'eight', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'r', 'p', 'g', 'la', 'm', 'n', 'v']\n",
      "seven ['six', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "while ['although', 'though', 'including', 'when', 'began', 'during', 'within', 'since']\n",
      "for ['when', 'against', 'above', 'without', 'if', 'through', 'concerning', 'while']\n",
      "s ['third', 'charles', 'second', 'isbn', 'iii', 'john', 'bob', 'state']\n",
      "used ['considered', 'seen', 'found', 'written', 'known', 'played', 'described', 'referred']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.102:  80%|██████████████████████████████████████████▍          | 160001/200001 [5:32:36<1:50:19,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'only', 'same', 'one', 'study']\n",
      "th ['nine', 'six', 'eight', 'seven', 'zero', 'five', 'bc', 'four']\n",
      "all ['both', 'many', 'these', 'every', 'some', 'each', 'several', 'what']\n",
      "about ['around', 'over', 'given', 'last', 'only', 'that', 'possible', 'out']\n",
      "as ['when', 'within', 'became', 'because', 'before', 'while', 'although', 'how']\n",
      "no ['any', 'another', 'only', 'than', 'possible', 'much', 'every', 'numbers']\n",
      "to ['would', 'will', 'may', 'can', 'him', 'not', 'against', 'must']\n",
      "been ['be', 'become', 'often', 'never', 'had', 'well', 'him', 'have']\n",
      "may ['can', 'could', 'would', 'will', 'must', 'should', 'might', 'cannot']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'michael', 'charles', 'r', 'm', 'c', 'p', 'la']\n",
      "seven ['six', 'eight', 'five', 'four', 'nine', 'zero', 'three', 'two']\n",
      "while ['although', 'though', 'began', 'since', 'during', 'however', 'including', 'before']\n",
      "for ['against', 'without', 'when', 'above', 'using', 'if', 'following', 'although']\n",
      "s ['third', 'second', 'charles', 'isbn', 'bob', 'series', 'iii', 'young']\n",
      "used ['found', 'known', 'seen', 'considered', 'written', 'played', 'held', 'referred']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.09:  85%|█████████████████████████████████████████████▉        | 170001/200001 [5:53:40<1:22:20,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'final', 'only', 'same', 'team']\n",
      "th ['nine', 'eight', 'six', 'seven', 'five', 'zero', 'bc', 'four']\n",
      "all ['both', 'many', 'these', 'some', 'every', 'several', 'only', 'use']\n",
      "about ['over', 'around', 'given', 'that', 'last', 'only', 'out', 'following']\n",
      "as ['within', 'because', 'when', 'although', 'became', 'before', 'while', 'particularly']\n",
      "no ['any', 'another', 'only', 'than', 'every', 'much', 'possible', 'even']\n",
      "to ['would', 'will', 'against', 'not', 'them', 'can', 'may', 'must']\n",
      "been ['be', 'become', 'well', 'never', 'often', 'had', 'him', 'already']\n",
      "may ['can', 'must', 'could', 'will', 'would', 'should', 'cannot', 'might']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'r', 'g', 'n', 'p', 'c', 'le', 'island']\n",
      "seven ['eight', 'six', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "while ['although', 'though', 'however', 'including', 'when', 'began', 'since', 'before']\n",
      "for ['when', 'without', 'although', 'if', 'against', 'following', 'while', 'above']\n",
      "s ['third', 'second', 'his', 'whose', 'edward', 'her', 'bob', 'your']\n",
      "used ['written', 'found', 'seen', 'known', 'considered', 'played', 'held', 'described']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.099:  90%|█████████████████████████████████████████████████▌     | 180001/200001 [6:14:50<56:52,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'final', 'same', 'only', 'team']\n",
      "th ['nine', 'eight', 'seven', 'six', 'five', 'zero', 'bc', 'four']\n",
      "all ['both', 'many', 'these', 'some', 'several', 'other', 'what', 'every']\n",
      "about ['over', 'around', 'given', 'on', 'following', 'last', 'that', 'through']\n",
      "as ['when', 'because', 'within', 'became', 'example', 'although', 'before', 'while']\n",
      "no ['any', 'another', 'only', 'than', 'every', 'even', 'possible', 'much']\n",
      "to ['would', 'will', 'must', 'should', 'may', 'not', 'could', 'can']\n",
      "been ['be', 'become', 'well', 'never', 'already', 'often', 'had', 'have']\n",
      "may ['can', 'must', 'could', 'would', 'will', 'should', 'cannot', 'might']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'r', 'g', 'v', 'c', 'iv', 'p', 'lord']\n",
      "seven ['six', 'eight', 'five', 'nine', 'four', 'zero', 'three', 'two']\n",
      "while ['although', 'though', 'when', 'however', 'before', 'began', 'since', 'until']\n",
      "for ['when', 'without', 'if', 'against', 'although', 'following', 'above', 'since']\n",
      "s ['third', 'second', 'her', 'whose', 'his', 'edward', 'single', 'of']\n",
      "used ['found', 'written', 'seen', 'considered', 'known', 'held', 'described', 'played']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.081:  95%|████████████████████████████████████████████████████▎  | 190001/200001 [6:36:00<28:23,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'final', 'same', 'latter', 'following']\n",
      "th ['bc', 'nine', 'eight', 'seven', 'six', 'five', 'zero', 'four']\n",
      "all ['both', 'many', 'these', 'some', 'several', 'only', 'every', 'various']\n",
      "about ['over', 'around', 'given', 'through', 'on', 'that', 'only', 'making']\n",
      "as ['within', 'because', 'when', 'too', 'although', 'example', 'using', 'particularly']\n",
      "no ['any', 'another', 'only', 'even', 'every', 'than', 'possible', 'much']\n",
      "to ['will', 'would', 'must', 'should', 'could', 'against', 'may', 'him']\n",
      "been ['be', 'become', 'well', 'already', 'often', 'had', 'never', 'sometimes']\n",
      "may ['can', 'could', 'must', 'will', 'would', 'should', 'cannot', 'might']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'island', 'la', 'r', 'king', 'et', 'g', 'j']\n",
      "seven ['six', 'eight', 'five', 'four', 'nine', 'zero', 'three', 'two']\n",
      "while ['although', 'though', 'however', 'when', 'since', 'before', 'began', 'until']\n",
      "for ['against', 'without', 'when', 'if', 'although', 'concerning', 'using', 'despite']\n",
      "s ['third', 'his', 'her', 'second', 'isbn', 'charles', 'your', 'edward']\n",
      "used ['seen', 'found', 'written', 'considered', 'described', 'known', 'played', 'defined']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.081: 100%|██████████████████████████████████████████████████████▉| 200000/200001 [6:57:17<00:00,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "first ['last', 'second', 'next', 'third', 'final', 'latter', 'following', 'fourth']\n",
      "th ['bc', 'nine', 'eight', 'seven', 'five', 'zero', 'six', 'nd']\n",
      "all ['both', 'many', 'these', 'some', 'several', 'various', 'what', 'each']\n",
      "about ['over', 'around', 'given', 'on', 'through', 'making', 'within', 'made']\n",
      "as ['because', 'example', 'too', 'within', 'when', 'although', 'using', 'instead']\n",
      "no ['any', 'another', 'only', 'every', 'even', 'much', 'than', 'little']\n",
      "to ['would', 'will', 'must', 'may', 'could', 'should', 'cannot', 'can']\n",
      "been ['be', 'become', 'already', 'often', 'well', 'recently', 'never', 'had']\n",
      "may ['can', 'could', 'will', 'must', 'would', 'should', 'cannot', 'might']\n",
      "six ['seven', 'eight', 'five', 'four', 'nine', 'three', 'zero', 'two']\n",
      "UNK ['l', 'la', 'le', 'r', 'et', 'v', 'g', 'j']\n",
      "seven ['six', 'eight', 'five', 'nine', 'four', 'three', 'zero', 'two']\n",
      "while ['although', 'though', 'when', 'however', 'but', 'despite', 'before', 'did']\n",
      "for ['without', 'when', 'against', 'if', 'using', 'although', 'concerning', 'once']\n",
      "s ['third', 'his', 'second', 'her', 'series', 'single', 'club', 'isbn']\n",
      "used ['found', 'seen', 'written', 'considered', 'described', 'defined', 'known', 'held']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.081: 100%|███████████████████████████████████████████████████████| 200001/200001 [6:57:17<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a path: ./final_model\n",
      "Created a path: ./checkpoints_nll/\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 189230], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3081 originated -> 5234 anarchism\n",
      "3081 originated -> 12 as\n",
      "12 as -> 6 a\n",
      "12 as -> 3081 originated\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/200001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "there ['trifluoride', 'aristophanes', 'manhattan', 'lau', 'brit', 'alternatives', 'kut', 'stagnation']\n",
      "or ['qazvin', 'toxins', 'baily', 'alekseyevich', 'moist', 'orry', 'tournament', 'ziegler']\n",
      "their ['usaid', 'reimbursement', 'ksl', 'yaffle', 'dragsters', 'linebarger', 'decrements', 'spinozism']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 11.514:   0%|                                                          | 1/200001 [00:00<13:47:22,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can ['beater', 'blackouts', 'anniversary', 'crop', 'evinced', 'verant', 'disallow', 'cronyism']\n",
      "nine ['bentleys', 'piercer', 'reingold', 'doron', 'modifications', 'kimberlite', 'offload', 'apabhramsha']\n",
      "the ['nehardea', 'minthorn', 'luncheon', 'latecomer', 'ttr', 'rectors', 'smears', 'urethra']\n",
      "b ['contracts', 'freeze', 'wakayama', 'warns', 'sforza', 'chronologies', 'tenochtitl', 'cockburn']\n",
      "however ['unpersuasive', 'meltwaters', 'rsvp', 'sympathomimetic', 'urstromtal', 'homepage', 'mandrakesoft', 'dusting']\n",
      "system ['sop', 'weidler', 'spermathecae', 'dena', 'drovers', 'transfers', 'garvey', 'retrospectively']\n",
      "united ['sentral', 'isle', 'counterpane', 'guti', 'gwalarn', 'cuthbert', 'covington', 'tich']\n",
      "first ['cpuid', 'longhorn', 'leacock', 'radioactive', 'poee', 'lampoons', 'initiatives', 'segregationist']\n",
      "five ['tumbuka', 'badougi', 'freebore', 'academy', 'taxonomically', 'earthbound', 'islote', 'talos']\n",
      "all ['mab', 'refashioned', 'madeira', 'aurelia', 'crags', 'louisa', 'poorly', 'vivo']\n",
      "his ['audio', 'cluster', 'attestations', 'prospective', 'ketfo', 'brauerei', 'verg', 'turboshaft']\n",
      "many ['jimsonweed', 'wrought', 'nicer', 'inbreeding', 'sextant', 'dagger', 'blankets', 'eliminated']\n",
      "d ['sheilding', 'aksharas', 'examiners', 'dugald', 'blaming', 'metabolism', 'nfta', 'nsaid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 6.852:  20%|███████████▎                                             | 39670/200001 [02:11<08:39, 308.55it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Avg loss: 6.873:  45%|█████████████████████████▋                               | 89922/200001 [04:59<06:17, 291.70it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Avg loss: 6.91:  50%|████████████████████████████▉                             | 99977/200001 [05:32<05:25, 306.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "there ['it', 'they', 'nor', 'typically', 'here', 'she', 'none', 'indeed']\n",
      "or ['drinking', 'than', 'and', 'though', 'meaning', 'normal', 'blood', 'serious']\n",
      "their ['its', 'our', 'his', 'your', 'my', 'her', 'whose', 'some']\n",
      "can ['could', 'must', 'may', 'cannot', 'should', 'will', 'might', 'would']\n",
      "nine ['seven', 'eight', 'six', 'ff', 'four', 'pmid', 'bb', 'circa']\n",
      "the ['buckingham', 'a', 'your', 'its', 'our', 'their', 'module', 'christianity']\n",
      "b ['d', 'f', 'c', 'r', 'q', 'beastie', 'jr', 'l']\n",
      "however ['but', 'although', 'though', 'while', 'nevertheless', 'indeed', 'nor', 'unless']\n",
      "system ['systems', 'process', 'technology', 'format', 'network', 'text', 'tree', 'program']\n",
      "united ['confederate', 'senate', 'netherlands', 'uss', 'sovereign', 'census', 'constitution', 'proto']\n",
      "first ['next', 'last', 'final', 'second', 'fourth', 'third', 'initially', 'latter']\n",
      "five ['four', 'seven', 'six', 'eight', 'three', 'two', 'nine', 'usd']\n",
      "all ['both', 'opposite', 'every', 'certain', 'atoms', 'plants', 'various', 'none']\n",
      "his ['her', 'my', 'their', 'your', 'our', 'whose', 'its', 's']\n",
      "many ['several', 'some', 'various', 'numerous', 'these', 'both', 'certain', 'those']\n",
      "d ['b', 'r', 'p', 'f', 'volo', 'e', 'ted', 'j']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 6.841:  70%|███████████████████████████████████████                 | 139553/200001 [07:43<03:18, 304.71it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Avg loss: 6.74:  85%|████████████████████████████████████████████████▍        | 169991/200001 [09:24<01:38, 304.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "there ['neither', 'they', 'it', 'none', 'forever', 'ultimately', 'unknown', 'indeed']\n",
      "or ['drinking', 'hole', 'and', 'whereas', 'sized', 'versus', 'than', 'solid']\n",
      "their ['its', 'your', 'his', 'our', 'her', 'my', 'whose', 'themselves']\n",
      "can ['must', 'could', 'cannot', 'should', 'might', 'will', 'may', 'would']\n",
      "nine ['seven', 'pmid', 'cet', 'eight', 'cvn', 'unassigned', 'cma', 'radios']\n",
      "the ['phase', 'connector', 'creationism', 'spontaneous', 'valign', 'consisting', 'drawing', 'conservation']\n",
      "b ['d', 'c', 'r', 'f', 'l', 'sh', 'j', 'p']\n",
      "however ['but', 'furthermore', 'indeed', 'although', 'additionally', 'nevertheless', 'moreover', 'though']\n",
      "system ['systems', 'process', 'technology', 'unit', 'program', 'network', 'methods', 'engine']\n",
      "united ['confederate', 'sovereign', 'papal', 'netherlands', 'senate', 'napoleonic', 'uss', 'crusader']\n",
      "first ['last', 'next', 'second', 'third', 'fourth', 'final', 'latter', 'longest']\n",
      "five ['seven', 'four', 'eight', 'six', 'three', 'unassigned', 'cvn', 'aleph']\n",
      "all ['both', 'various', 'these', 'multiple', 'certain', 'none', 'estimates', 'every']\n",
      "his ['her', 'my', 'their', 'your', 'our', 'claudius', 'its', 'whose']\n",
      "many ['several', 'some', 'numerous', 'various', 'few', 'those', 'multiple', 'certain']\n",
      "d ['b', 'j', 'max', 'p', 'f', 'r', 'h', 'l']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 6.729:  88%|█████████████████████████████████████████████████▌      | 176971/200001 [09:47<01:16, 302.83it/s]"
     ]
    }
   ],
   "source": [
    "run_training(\n",
    "    model_type = 'neg', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 1, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 1, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    ")\n",
    "run_training(\n",
    "    model_type = 'nll', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 1, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 1, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KxsyQ8gsF-0"
   },
   "source": [
    "## Testing Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zswvfdgesjeF"
   },
   "source": [
    "<b>Analogies using word vectors</b>\n",
    "\n",
    "Use the word vectors learned from both approaches in the following word analogy task.\n",
    "\n",
    "Each question/task is in the following form.\n",
    "```\n",
    "Consider the following word pairs that share the same relation, R:\n",
    "\n",
    "    pilgrim:shrine, hunter:quarry, assassin:victim, climber:peak\n",
    "\n",
    "Among these word pairs,\n",
    "\n",
    "(1) pig:mud\n",
    "(2) politician:votes\n",
    "(3) dog:bone\n",
    "(4) bird:worm\n",
    "\n",
    "Q1. Which word pairs has the MOST illustrative(similar) example of the relation R?\n",
    "Q2. Which word pairs has the LEAST illustrative(similar) example of the relation R?\n",
    "```\n",
    "\n",
    "For each question, there are examples pairs of a certain relation. The task is to find the most/least illustrative word pair of the relation. One simple method to answer those questions will be measuring the similarities of difference vectors.\n",
    "\n",
    "Vectors are representing some direction in space. If (a, b) and (c, d) pairs are analogous pairs then the transformation from a to b (i.e., some x vector when added to a gives b: a + x = b) should be highly similar to the transformation from c to d (i.e., some y vector when added to c gives d: c + y = d). In other words, the difference vector (b-a) should be similar to difference vector (d-c).\n",
    "\n",
    "This difference vector can be thought to represent the relation between the two words.\n",
    "\n",
    "Due to the noisy annotation data, the expected accuracy is not high. The NLL default overall accuracy is 33.5% and negative sampling default overall accuracy is 33.6%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUD-elYkPi_y"
   },
   "source": [
    "<b>Further implementation explanation:</b>\n",
    "\n",
    "  - `In the next 2 cells`:\n",
    "    Evaluating relation between pairs of words -- called the [MaxDiff question](https://en.wikipedia.org/wiki/MaxDiff).\n",
    "    Generate a file with the predictions following the format of `word_analogy_sample_predictions.txt`.\n",
    "\n",
    "  - `evaluate_word_analogy.pl`:\n",
    "    This is a perl script to evaluate THE PREDICTIONS on development data. Use it as shown in the next cell. \n",
    "\n",
    "  - `word_analogy_dev.txt`:\n",
    "    This is some data for development.\n",
    "    Each line of this file is divided into \"examples\" and \"choices\" by \"||\".\n",
    "        [examples]||[choices]\n",
    "    \"Examples\" and \"choices\" are delimited by a comma.\n",
    "      For example:  \"tailor:suit\",\"oracle:prophesy\",\"baker:flour\"\n",
    "\n",
    "  - `word_analogy_dev_sample_predictions.txt`:\n",
    "    A sample prediction file. Pay attention to the format of this file.\n",
    "    The prediction file follows this to use \"score_maxdiff.pl\" script.\n",
    "    Each row is in this format:\n",
    "    \n",
    "      <pair1> <pair2> <pair3> <pair4> <least_illustrative_pair> <most_illustrative_pair>\n",
    "\n",
    "    The order of word pairs matchs their original order found in `word_analogy_dev.txt`.\n",
    "\n",
    "  - `word_analogy_dev_mturk_answers.txt`:\n",
    "    This is the answers collected using Amazon mechanical turk for `word_analogy_dev.txt`.\n",
    "    The answers in this file is used as the correct answer and used to evaluate the analogy predictions. (using \"evaluate_word_analogy.pl\")\n",
    "\n",
    "  - `word_analogy_test.txt`:\n",
    "    Test data file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Xg-sE4hfsJxz"
   },
   "outputs": [],
   "source": [
    "def read_data_analogy(file_path): # NAME MODIFIED FOR BETTER EXPERIMENT EXPERIENCE\n",
    "    with open(file_path,'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    candidate, test = [], []\n",
    "    for line in data:\n",
    "        a, b = line.strip().split(\"||\")\n",
    "        a = [i[1:-1].split(\":\") for i in a.split(\",\")]\n",
    "        b = [i[1:-1].split(\":\") for i in b.split(\",\")]\n",
    "        candidate.append(a)\n",
    "        test.append(b)\n",
    "\n",
    "    return candidate, test\n",
    "\n",
    "def get_embeddings(examples, embeddings, dictionary):\n",
    "\n",
    "    \"\"\"\n",
    "    For the word pairs in the 'examples' array, fetch embeddings and return.\n",
    "    You can access your trained model via dictionary and embeddings.\n",
    "    dictionary[word] will give you word_id\n",
    "    and embeddings[word_id] will return the embedding for that word.\n",
    "\n",
    "    word_id = dictionary[word]\n",
    "    v1 = embeddings[word_id]\n",
    "\n",
    "    or simply\n",
    "\n",
    "    v1 = embeddings[dictionary[word_id]]\n",
    "    \"\"\"\n",
    "\n",
    "    norm = np.sqrt(np.sum(np.square(embeddings),axis=1,keepdims=True))\n",
    "    normalized_embeddings = embeddings/norm\n",
    "\n",
    "    embs = []\n",
    "    for line in examples:\n",
    "        temp = []\n",
    "        for pairs in line:\n",
    "            temp.append([ normalized_embeddings[dictionary[pairs[0]]], normalized_embeddings[dictionary[pairs[1]]] ])\n",
    "        embs.append(temp)\n",
    "\n",
    "    result = np.array(embs)\n",
    "\n",
    "    return result\n",
    "\n",
    "def evaluate_pairs(candidate_embs, test_embs):\n",
    "\n",
    "    \"\"\"\n",
    "    Write code to evaluate a relation between pairs of words.\n",
    "    Find the best and worst pairs and return that.\n",
    "    \"\"\"\n",
    "\n",
    "    best_pairs = []\n",
    "    worst_pairs = []\n",
    "\n",
    "    #TODO:\n",
    "    #print(\"candidate_embs\\n\")\n",
    "    #print(candidate_embs[:2])\n",
    "    #print(\"test_embs\\n\")\n",
    "    #print(test_embs[:3])\n",
    "    candidate_embs = np.array(candidate_embs)\n",
    "    test_embs = np.array(test_embs)\n",
    "    diff_of_candidate = np.zeros((len(candidate_embs),len(candidate_embs[0]),\n",
    "                                  len(candidate_embs[0][0][0])),\n",
    "                                 dtype = np.float32)\n",
    "    for i in range (len(candidate_embs)):\n",
    "      for j in range (len(candidate_embs[0])):\n",
    "        diff_of_candidate[i][j]=np.array(candidate_embs[i][j][0]-candidate_embs[i][j][1])\n",
    "    diff_of_test = test_embs [:,:,0,:] - test_embs [:,:,1,:]\n",
    "    #print(\"candidate_diff\\n\")\n",
    "    #print(diff_of_candidate[:3])\n",
    "    #print(\"test_diff\\n\")\n",
    "    #print(diff_of_test[:3])\n",
    "    for i, line in enumerate(diff_of_test):\n",
    "      #print()\n",
    "      #print(\"line\")\n",
    "      #print(line)\n",
    "      line_test_pairs_score = []\n",
    "      for j, pair in enumerate(diff_of_test[i]):\n",
    "        #print(\"pair\")\n",
    "        #print(pair)\n",
    "        pair_dot_sum = 0\n",
    "        for k, cadidate_pair in enumerate(diff_of_candidate[i]):\n",
    "          pair_dot_sum = pair_dot_sum + np.dot(pair,cadidate_pair)\n",
    "          #print (\"pair_dot_sum\")\n",
    "          #print (pair_dot_sum)\n",
    "        line_test_pairs_score.append(pair_dot_sum)\n",
    "      #print(line_test_pairs_score)\n",
    "      min = np.argmin(line_test_pairs_score)\n",
    "      max = np.argmax(line_test_pairs_score)\n",
    "      #print(min)\n",
    "      #print(max)\n",
    "      best_pairs.append(max)\n",
    "      worst_pairs.append(min)\n",
    "\n",
    "    return best_pairs, worst_pairs\n",
    "def write_solution(best_pairs, worst_pairs, test, path):\n",
    "\n",
    "    \"\"\"\n",
    "    Write best and worst pairs to a file, that can be evaluated by evaluate_word_analogy.pl\n",
    "    \"\"\"\n",
    "\n",
    "    ans = []\n",
    "    for i, line in enumerate(test):\n",
    "        temp = [f'\"{pairs[0]}:{pairs[1]}\"' for pairs in line]\n",
    "        temp.append(f'\"{line[worst_pairs[i]][0]}:{line[worst_pairs[i]][1]}\"')\n",
    "        temp.append(f'\"{line[best_pairs[i]][0]}:{line[best_pairs[i]][1]}\"')\n",
    "        ans.append(\" \".join(temp))\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"\\n\".join(ans))\n",
    "\n",
    "\n",
    "def run_word_analogy_eval(\n",
    "    model_path = './final_model', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_demo_results.txt', # predicted results\n",
    "    model_type = 'nll' # type of model being used, NLL or NEG\n",
    "):\n",
    "\n",
    "    print(f'Model file: {model_path}/word2vec_{model_type}.model')\n",
    "    model_filepath = os.path.join(model_path, 'word2vec_%s.model'%(model_type))\n",
    "\n",
    "    dictionary, embeddings = pickle.load(open(model_filepath, 'rb'))\n",
    "\n",
    "    candidate, test = read_data_analogy(input_filepath) # READ DATA NAME MODIFIED\n",
    "\n",
    "    candidate_embs = get_embeddings(candidate, embeddings, dictionary)\n",
    "    test_embs = get_embeddings(test, embeddings, dictionary)\n",
    "\n",
    "    best_pairs, worst_pairs = evaluate_pairs(candidate_embs, test_embs)\n",
    "\n",
    "    out_filepath = output_filepath\n",
    "    print(f'Output file: {out_filepath}')\n",
    "    write_solution(best_pairs, worst_pairs, test, out_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ui1NrgB8Q1b8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "46Rzv8Q6wVS4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file: ./final_demo_model/word2vec_neg.model\n",
      "Output file: word_analogy_demo_dev_results.txt\n"
     ]
    }
   ],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_demo_model', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_demo_dev_results.txt', # predicted results\n",
    "    model_type = 'neg' # type of model being used, NLL or NEG\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXBKXZvcMVN6"
   },
   "source": [
    "The results can finally be converted into numeric metrics using the Perl script below. A demo score result is also provided for refernce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "jLBe5k_DGs1_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!chmod 777 evaluate_word_analogy.pl\n",
    "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_demo_dev_results.txt demo_score_neg.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'perl' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !perl evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_demo_dev_results.txt > demo_score_neg.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqgKOnZ7Qg8x"
   },
   "source": [
    "## Running Experiments with various hyper-parameters for the Neg models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Kf1fWWnYH6H"
   },
   "source": [
    "There are five experiments, where each experiment involves learning word vectors using Negative Sampling model with a specific setting for the three hyper parameters listed below and evaluating the resulting word vectors on the test set of the word analogy task.\n",
    "\n",
    "Hyper parameters to try:\n",
    "  - Number of Neg samples (Can vary from 1 to 5)\n",
    "  - Learning Rate (Can vary from 0.1 to 10)\n",
    "  - Window size (Can vary from 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m0Q604fnADQ"
   },
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACsHlAnDnj2W"
   },
   "source": [
    "* Number of Neg samples 1\n",
    "\n",
    "* Learning Rate 1\n",
    "\n",
    "* Window size 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a0ui6zEneGV"
   },
   "source": [
    "Expectation: more accurate than demo, take longer time then demo, overal accuracy approach 33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6DIPVJYFQYMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a path: ./checkpoints_1_neg/\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 189230], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3081 originated -> 12 as\n",
      "3081 originated -> 5234 anarchism\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 1/200001 [00:00<21:30:27,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['boku', 'handball', 'mnr', 'nukem', 'resisters', 'scarcity', 'harare', 'variation']\n",
      "a ['wiggin', 'enfranchise', 'foreclosure', 'maximal', 'seeking', 'lyrae', 'baba', 'optimal']\n",
      "i ['bedloe', 'hahn', 'ottokar', 'retrocomputing', 'hergest', 'surrogate', 'bambino', 'loudest']\n",
      "to ['fermo', 'dunaliella', 'asamblea', 'technologie', 'geb', 'wattles', 'nephrotic', 'fil']\n",
      "an ['allchin', 'demons', 'breastbone', 'nikolay', 'sayers', 'snubbed', 'trader', 'nathaniel']\n",
      "or ['zechs', 'tramway', 'abbeys', 'northumbria', 'crests', 'accompanist', 'stunts', 'rnsen']\n",
      "use ['psr', 'inconvenience', 'scenarios', 'barbie', 'cilician', 'argand', 'dooagh', 'fairway']\n",
      "th ['uncouth', 'zionist', 'rivest', 'hasten', 'anglers', 'acapulco', 'infamously', 'yeshiva']\n",
      "in ['waukegan', 'page', 'turbo', 'psychoanalyst', 'petronius', 'jurisdiction', 'croisset', 'surfin']\n",
      "united ['coarse', 'pomorze', 'nasi', 'prayerful', 'wee', 'valderrama', 'opportunity', 'dvora']\n",
      "there ['trawling', 'wavpack', 'lorien', 'china', 'diablospeak', 'incendiary', 'hawai', 'gambler']\n",
      "will ['mci', 'kazuo', 'kuching', 'chlorian', 'thousanders', 'crawling', 'imitative', 'whitney']\n",
      "can ['intelligently', 'remarked', 'andhra', 'generalis', 'moana', 'gadu', 'codebooks', 'halves']\n",
      "most ['visarga', 'gundestrup', 'gorani', 'alien', 'kopf', 'sampson', 'berglin', 'locating']\n",
      "up ['promulgation', 'supercomputer', 'lihou', 'restarting', 'oxidizes', 'holding', 'escapism', 'laure']\n",
      "only ['thunderstorm', 'hlinka', 'jh', 'wea', 'adulterers', 'monounsaturated', 'livio', 'species']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.258:   5%|██▊                                                     | 10001/200001 [21:13<8:30:21,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'is', 'was', 'would', 'has', 'had', 'nukem', 'furse']\n",
      "a ['the', 'this', 'an', 'its', 'his', 'ferncliff', 'known', 'terraserver']\n",
      "i ['simpler', 'romansh', 'csp', 'focused', 'uterine', 'belittle', 'buell', 'opie']\n",
      "to ['would', 'with', 'in', 'owe', 'fermo', 'can', 'from', 'longitudinally']\n",
      "an ['this', 'its', 'a', 'his', 'the', 'their', 'some', 'her']\n",
      "or ['for', 'of', 'is', 'agave', 'at', 'subterranean', 'patches', 'and']\n",
      "use ['planning', 'east', 'end', 'analysis', 'center', 'result', 'argand', 'them']\n",
      "th ['six', 'three', 'five', 'eight', 'four', 'seven', 'metres', 'zero']\n",
      "in ['from', 'of', 'for', 'on', 'by', 'with', 'at', 's']\n",
      "united ['however', 'karoline', 'iweb', 'maury', 'lma', 'recurring', 'asinus', 'machpelah']\n",
      "there ['he', 'not', 'also', 'it', 'often', 'which', 'they', 'who']\n",
      "will ['would', 'can', 'speakers', 'mci', 'may', 'accumulate', 'url', 'pym']\n",
      "can ['would', 'may', 'could', 'will', 'should', 'to', 'was', 'mellow']\n",
      "most ['family', 'ghanaians', 'beholden', 'blurs', 'bundesrepublik', 'dotless', 'disbelievers', 'handsfree']\n",
      "up ['octave', 'north', 'due', 'promulgation', 'oxidizes', 'nonfiction', 'diamonds', 'spain']\n",
      "only ['back', 'salientia', 'species', 'agouti', 'expansive', 'thought', 'canadian', 'area']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.218:  10%|█████▌                                                  | 20001/200001 [41:36<8:11:20,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'was', 'is', 'had', 'has', 'became', 'if', 'although']\n",
      "a ['the', 'its', 'his', 'delroy', 'an', 'known', 'this', 'ferncliff']\n",
      "i ['he', 'focused', 'she', 'fabrication', 'they', 'chamorro', 'it', 'romansh']\n",
      "to ['would', 'against', 'with', 'owe', 'fermo', 'can', 'cleaners', 'thickly']\n",
      "an ['this', 'its', 'known', 'their', 'her', 'the', 'his', 'some']\n",
      "or ['for', 'with', 'became', 'and', 'at', 'like', 'than', 'is']\n",
      "use ['planning', 'equivalent', 'command', 'describe', 'process', 'them', 'equation', 'chess']\n",
      "th ['five', 'eight', 'bc', 'seven', 'six', 'three', 'four', 'million']\n",
      "in ['from', 'on', 'at', 'for', 'by', 'with', 'between', 'into']\n",
      "united ['iweb', 'lma', 'maury', 'asinus', 'karoline', 'begetteth', 'lameness', 'ashina']\n",
      "there ['he', 'it', 'who', 'no', 'they', 'still', 'often', 'not']\n",
      "will ['would', 'can', 'should', 'could', 'may', 'then', 'speakers', 'might']\n",
      "can ['would', 'may', 'could', 'should', 'will', 'must', 'might', 'had']\n",
      "most ['bundesrepublik', 'ghanaians', 'beholden', 'coast', 'elements', 'family', 'without', 'uvular']\n",
      "up ['octave', 'spain', 'region', 'limited', 'developed', 'present', 'grinnellplans', 'holding']\n",
      "only ['tomatoes', 'attention', 'just', 'peterson', 'mission', 'but', 'salientia', 'better']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.175:  15%|████████                                              | 30001/200001 [1:02:03<7:48:11,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'is', 'was', 'had', 'became', 'has', 'have', 'if']\n",
      "a ['the', 'its', 'delroy', 'his', 'this', 'ferncliff', 'another', 'renegade']\n",
      "i ['focused', 'roast', 'bartos', 'god', 'inflation', 'g', 'he', 'skies']\n",
      "to ['with', 'against', 'into', 'would', 'from', 'fermo', 'thickly', 'through']\n",
      "an ['this', 'the', 'dramatically', 'their', 'her', 'his', 'its', 'it']\n",
      "or ['than', 'and', 'modern', 'of', 'for', 'in', 'haliotis', 'korchnoi']\n",
      "use ['equivalent', 'resist', 'describe', 'equation', 'planning', 'venice', 'rivers', 'tackling']\n",
      "th ['bc', 'six', 'five', 'million', 'three', 'eight', 'seven', 'four']\n",
      "in ['from', 'on', 'at', 'for', 'between', 'with', 'until', 'by']\n",
      "united ['australian', 'crossroad', 'near', 'lma', 'iweb', 'maury', 'gregorian', 'great']\n",
      "there ['they', 'he', 'it', 'still', 'who', 'no', 'often', 'also']\n",
      "will ['can', 'would', 'should', 'could', 'might', 'may', 'then', 'must']\n",
      "can ['would', 'could', 'should', 'will', 'may', 'must', 'might', 'then']\n",
      "most ['family', 'bundesrepublik', 'coast', 'name', 'beholden', 'uvular', 'elements', 'process']\n",
      "up ['him', 'octave', 'come', 'directly', 'limited', 'applied', 'due', 'grinnellplans']\n",
      "only ['late', 'better', 'until', 'tomatoes', 'just', 'now', 'considered', 'made']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.168:  20%|██████████▊                                           | 40001/200001 [1:22:37<7:10:40,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'was', 'is', 'had', 'has', 'became', 'have', 'although']\n",
      "a ['delroy', 'the', 'another', 'no', 'this', 'hypertrophy', 'inductees', 'its']\n",
      "i ['god', 'he', 'it', 'they', 'ii', 'who', 'then', 'bartos']\n",
      "to ['would', 'against', 'into', 'not', 'with', 'fermo', 'may', 'from']\n",
      "an ['their', 'the', 'dramatically', 'her', 'this', 'his', 'its', 'some']\n",
      "or ['and', 'than', 'of', 'modern', 'at', 'for', 'like', 'haliotis']\n",
      "use ['student', 'describe', 'resist', 'means', 'remove', 'make', 'procedures', 'think']\n",
      "th ['bc', 'five', 'three', 'six', 'seven', 'eight', 'four', 'july']\n",
      "in ['from', 'on', 'at', 'between', 'during', 'under', 'for', 'after']\n",
      "united ['australian', 'crossroad', 'great', 'national', 'church', 'unabridged', 'confederate', 'paige']\n",
      "there ['it', 'he', 'they', 'still', 'who', 'she', 'often', 'again']\n",
      "will ['would', 'can', 'could', 'should', 'might', 'may', 'must', 'cannot']\n",
      "can ['would', 'could', 'may', 'should', 'will', 'must', 'might', 'cannot']\n",
      "most ['many', 'production', 'coast', 'downstream', 'beholden', 'shock', 'process', 'name']\n",
      "up ['him', 'supercomputer', 'octave', 'accomplish', 'limited', 'permitted', 'directly', 'to']\n",
      "only ['now', 'it', 'better', 'still', 'just', 'beatles', 'until', 'probably']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.156:  25%|█████████████▍                                        | 50000/200001 [1:43:06<5:06:22,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'was', 'had', 'is', 'have', 'became', 'has', 'while']\n",
      "a ['the', 'delroy', 'another', 'any', 'part', 'no', 'its', 'ferncliff']\n",
      "i ['they', 'p', 'it', 'who', 'he', 'she', 'we', 'ii']\n",
      "to ['would', 'not', 'may', 'can', 'against', 'into', 'up', 'through']\n",
      "an ['it', 'dramatically', 'the', 'their', 'her', 'this', 'radicalism', 'cremello']\n",
      "or ['than', 'and', 'haliotis', 'modern', 'notting', 'subterranean', 'beechcraft', 'harmonics']\n",
      "use ['be', 'divine', 'student', 'process', 'name', 'command', 'nig', 'remove']\n",
      "th ['bc', 'five', 'six', 'eight', 'seven', 'four', 'three', 'nine']\n",
      "in ['on', 'at', 'from', 'between', 'under', 'during', 'through', 'within']\n",
      "united ['two', 'australian', 'nine', 'eight', 'seven', 'four', 'great', 'city']\n",
      "there ['he', 'they', 'it', 'she', 'who', 'still', 'often', 'later']\n",
      "will ['would', 'can', 'should', 'could', 'might', 'may', 'must', 'cannot']\n",
      "can ['would', 'could', 'may', 'will', 'should', 'must', 'might', 'cannot']\n",
      "most ['beholden', 'downstream', 'leveling', 'for', 'production', 'use', 'shock', 'also']\n",
      "up ['him', 'to', 'supercomputer', 'back', 'octave', 'their', 'segmenting', 'limited']\n",
      "only ['now', 'it', 'better', 'not', 'beatles', 'just', 'also', 'but']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.167:  30%|████████████████▏                                     | 60001/200001 [2:03:38<6:20:12,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'was', 'have', 'had', 'is', 'been', 'became', 'has']\n",
      "a ['another', 'any', 'no', 'delroy', 'its', 'known', 'part', 'this']\n",
      "i ['g', 'p', 'ii', 'f', 'w', 'c', 'r', 'he']\n",
      "to ['would', 'can', 'may', 'not', 'into', 'through', 'up', 'against']\n",
      "an ['dramatically', 'the', 'their', 'it', 'its', 'known', 'this', 'radicalism']\n",
      "or ['than', 'and', 'modern', 'each', 'a', 'notting', 'for', 'lxii']\n",
      "use ['be', 'most', 'name', 'end', 'example', 'them', 'used', 'result']\n",
      "th ['eight', 'bc', 'six', 'five', 'seven', 'zero', 'four', 'nine']\n",
      "in ['at', 'from', 'on', 'under', 'between', 'during', 'against', 'within']\n",
      "united ['one', 'eight', 'six', 'nine', 'three', 'two', 'same', 'four']\n",
      "there ['he', 'it', 'they', 'she', 'still', 'who', 'said', 'now']\n",
      "will ['would', 'can', 'could', 'should', 'may', 'must', 'might', 'cannot']\n",
      "can ['would', 'could', 'may', 'will', 'must', 'should', 'might', 'cannot']\n",
      "most ['use', 'many', 'also', 'more', 'are', 'other', 'name', 'production']\n",
      "up ['him', 'to', 'back', 'their', 'them', 'place', 'down', 'her']\n",
      "only ['it', 'now', 'not', 'but', 'also', 'then', 'still', 'there']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.163:  35%|██████████████████▉                                   | 70001/200001 [2:24:20<6:22:51,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'was', 'had', 'have', 'been', 'became', 'is', 'has']\n",
      "a ['another', 'any', 'very', 'delroy', 'known', 'the', 'no', 'more']\n",
      "i ['you', 'we', 'she', 'they', 'who', 'he', 'p', 'god']\n",
      "to ['would', 'may', 'can', 'not', 'will', 'could', 'up', 'must']\n",
      "an ['dramatically', 'it', 'their', 'known', 'motivations', 'its', 'beverages', 'salted']\n",
      "or ['than', 'and', 'modern', 'including', 'like', 'each', 'alternative', 'korchnoi']\n",
      "use ['name', 'be', 'end', 'them', 'example', 'used', 'result', 'process']\n",
      "th ['five', 'eight', 'six', 'seven', 'bc', 'zero', 'nine', 'three']\n",
      "in ['on', 'under', 'at', 'within', 'during', 'from', 'between', 'until']\n",
      "united ['eight', 'nine', 'four', 'five', 'two', 'seven', 'zero', 'one']\n",
      "there ['it', 'he', 'they', 'she', 'said', 'still', 'zero', 'six']\n",
      "will ['would', 'can', 'could', 'should', 'must', 'may', 'might', 'cannot']\n",
      "can ['would', 'could', 'will', 'may', 'must', 'should', 'might', 'cannot']\n",
      "most ['use', 'more', 'many', 'are', 'other', 'name', 'being', 'also']\n",
      "up ['him', 'them', 'back', 'to', 'their', 'place', 'her', 'out']\n",
      "only ['it', 'not', 'now', 'but', 'around', 'then', 'better', 'still']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.152:  40%|█████████████████████▌                                | 80001/200001 [2:44:53<5:30:07,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'was', 'had', 'became', 'been', 'be', 'has']\n",
      "a ['another', 'the', 'any', 'no', 'known', 'part', 'very', 'one']\n",
      "i ['you', 'we', 'p', 'ii', 'they', 'god', 'who', 'she']\n",
      "to ['would', 'may', 'will', 'can', 'not', 'must', 'into', 'could']\n",
      "an ['dramatically', 'it', 'its', 'known', 'their', 'motivations', 'stang', 'radicalism']\n",
      "or ['than', 'and', 'modern', 'like', 'each', 'hence', 'alternative', 'including']\n",
      "use ['end', 'be', 'form', 'example', 'nine', 'used', 'eight', 'all']\n",
      "th ['six', 'five', 'eight', 'seven', 'zero', 'four', 'three', 'nine']\n",
      "in ['under', 'within', 'during', 'on', 'at', 'between', 'from', 'until']\n",
      "united ['eight', 'nine', 'five', 'seven', 'six', 'four', 'zero', 'city']\n",
      "there ['it', 'they', 'he', 'she', 'zero', 'nine', 'still', 'six']\n",
      "will ['would', 'can', 'could', 'must', 'should', 'may', 'might', 'cannot']\n",
      "can ['would', 'could', 'will', 'may', 'must', 'should', 'might', 'cannot']\n",
      "most ['more', 'people', 'use', 'other', 'many', 'also', 'are', 'both']\n",
      "up ['him', 'them', 'back', 'place', 'out', 'their', 'her', 'not']\n",
      "only ['it', 'around', 'now', 'not', 'then', 'also', 'just', 'over']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.139:  45%|████████████████████████▎                             | 90001/200001 [3:05:28<4:54:51,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'was', 'had', 'be', 'been', 'while', 'became']\n",
      "a ['another', 'the', 'any', 'very', 'each', 'no', 'first', 'number']\n",
      "i ['you', 'we', 'they', 'ii', 'p', 'he', 'she', 'h']\n",
      "to ['would', 'not', 'will', 'can', 'may', 'must', 'could', 'up']\n",
      "an ['dramatically', 'its', 'it', 'their', 'motivations', 'salted', 'stang', 'play']\n",
      "or ['than', 'and', 'like', 'modern', 'each', 'non', 'alternative', 'including']\n",
      "use ['end', 'example', 'form', 'nine', 'seven', 'eight', 'name', 'zero']\n",
      "th ['eight', 'six', 'seven', 'five', 'four', 'zero', 'nine', 'three']\n",
      "in ['during', 'under', 'within', 'between', 'at', 'from', 'on', 'until']\n",
      "united ['nine', 'eight', 'six', 'seven', 'five', 'zero', 'four', 'one']\n",
      "there ['it', 'they', 'still', 'he', 'she', 'said', 'zero', 'nine']\n",
      "will ['would', 'can', 'could', 'must', 'may', 'should', 'might', 'cannot']\n",
      "can ['would', 'could', 'will', 'must', 'may', 'should', 'might', 'cannot']\n",
      "most ['more', 'other', 'use', 'many', 'people', 'also', 'both', 'known']\n",
      "up ['him', 'them', 'out', 'back', 'over', 'place', 'down', 'not']\n",
      "only ['around', 'it', 'over', 'but', 'just', 'not', 'now', 'also']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.124:  50%|██████████████████████████▍                          | 100000/200001 [3:26:18<3:22:50,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'had', 'was', 'been', 'be', 'became', 'has']\n",
      "a ['another', 'the', 'any', 'very', 'each', 'no', 'its', 'delroy']\n",
      "i ['you', 'ii', 'god', 'we', 'p', 'k', 'g', 'he']\n",
      "to ['would', 'will', 'not', 'may', 'can', 'against', 'through', 'up']\n",
      "an ['dramatically', 'motivations', 'it', 'play', 'group', 'its', 'salted', 'their']\n",
      "or ['than', 'like', 'non', 'modern', 'and', 'okina', 'plantagenet', 'good']\n",
      "use ['nine', 'eight', 'zero', 'end', 'order', 'death', 'seven', 'book']\n",
      "th ['five', 'eight', 'seven', 'six', 'nine', 'zero', 'four', 'one']\n",
      "in ['during', 'under', 'within', 'since', 'at', 'between', 'around', 'until']\n",
      "united ['nine', 'eight', 'seven', 'six', 'zero', 'four', 'five', 'three']\n",
      "there ['it', 'they', 'he', 'still', 'she', 'zero', 'eight', 'six']\n",
      "will ['would', 'can', 'could', 'may', 'must', 'should', 'might', 'cannot']\n",
      "can ['would', 'will', 'could', 'may', 'must', 'should', 'might', 'cannot']\n",
      "most ['more', 'use', 'other', 'many', 'both', 'people', 'some', 'also']\n",
      "up ['him', 'them', 'back', 'out', 'over', 'place', 'down', 'off']\n",
      "only ['around', 'but', 'also', 'still', 'now', 'it', 'not', 'just']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.125:  55%|█████████████████████████████▏                       | 110001/200001 [3:46:59<4:07:51,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'had', 'was', 'been', 'became', 'be', 'being']\n",
      "a ['another', 'any', 'very', 'no', 'the', 'each', 'number', 'every']\n",
      "i ['you', 'we', 'ii', 'god', 'they', 'then', 'g', 'p']\n",
      "to ['would', 'will', 'not', 'through', 'may', 'up', 'against', 'could']\n",
      "an ['dramatically', 'group', 'motivations', 'play', 'stang', 'it', 'made', 'allowed']\n",
      "or ['than', 'and', 'like', 'non', 'good', 'modern', 'plantagenet', 'any']\n",
      "use ['nine', 'do', 'form', 'zero', 'end', 'order', 'eight', 'used']\n",
      "th ['eight', 'five', 'six', 'seven', 'nine', 'four', 'zero', 'bc']\n",
      "in ['during', 'under', 'within', 'until', 'near', 'since', 'between', 'through']\n",
      "united ['nine', 'eight', 'seven', 'four', 'zero', 'five', 'six', 'battle']\n",
      "there ['it', 'they', 'still', 'zero', 'said', 'she', 'possible', 'he']\n",
      "will ['would', 'can', 'could', 'must', 'may', 'should', 'might', 'cannot']\n",
      "can ['will', 'would', 'could', 'must', 'may', 'should', 'might', 'cannot']\n",
      "most ['more', 'use', 'many', 'other', 'some', 'both', 'people', 'country']\n",
      "up ['him', 'them', 'out', 'back', 'off', 'over', 'down', 'place']\n",
      "only ['around', 'but', 'also', 'still', 'usually', 'not', 'there', 'now']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.116:  60%|███████████████████████████████▊                     | 120001/200001 [4:07:45<3:35:52,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'had', 'been', 'was', 'became', 'be', 'although']\n",
      "a ['another', 'any', 'very', 'each', 'no', 'every', 'number', 'range']\n",
      "i ['you', 'we', 'ii', 'they', 'god', 'p', 'g', 'then']\n",
      "to ['would', 'will', 'against', 'not', 'through', 'may', 'could', 'up']\n",
      "an ['dramatically', 'group', 'motivations', 'stang', 'salted', 'beverages', 'allowed', 'their']\n",
      "or ['than', 'and', 'okina', 'though', 'like', 'libanius', 'non', 'plantagenet']\n",
      "use ['form', 'end', 'case', 'result', 'example', 'order', 'nine', 'used']\n",
      "th ['eight', 'five', 'six', 'nine', 'seven', 'zero', 'four', 'bc']\n",
      "in ['within', 'during', 'under', 'around', 'until', 'between', 'since', 'through']\n",
      "united ['nine', 'six', 'eight', 'seven', 'zero', 'five', 'battle', 'four']\n",
      "there ['it', 'they', 'still', 'said', 'he', 'she', 'six', 'however']\n",
      "will ['would', 'can', 'could', 'may', 'must', 'should', 'might', 'cannot']\n",
      "can ['will', 'would', 'may', 'could', 'must', 'should', 'might', 'cannot']\n",
      "most ['more', 'use', 'many', 'other', 'both', 'used', 'some', 'very']\n",
      "up ['him', 'them', 'back', 'off', 'out', 'down', 'over', 'place']\n",
      "only ['around', 'usually', 'also', 'now', 'still', 'not', 'but', 'possible']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.105:  65%|██████████████████████████████████▍                  | 130001/200001 [4:28:50<3:12:10,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'been', 'had', 'became', 'was', 'be', 'being']\n",
      "a ['another', 'any', 'each', 'very', 'no', 'the', 'range', 'known']\n",
      "i ['you', 'ii', 'we', 'god', 'p', 'g', 'k', 'f']\n",
      "to ['would', 'will', 'against', 'may', 'through', 'not', 'could', 'can']\n",
      "an ['dramatically', 'group', 'seen', 'motivations', 'what', 'stang', 'salted', 'it']\n",
      "or ['though', 'and', 'including', 'than', 'while', 'low', 'like', 'modern']\n",
      "use ['form', 'example', 'zero', 'case', 'nine', 'end', 'eight', 'order']\n",
      "th ['five', 'eight', 'six', 'seven', 'nine', 'zero', 'four', 'bc']\n",
      "in ['within', 'during', 'under', 'around', 'until', 'throughout', 'since', 'among']\n",
      "united ['nine', 'seven', 'five', 'eight', 'zero', 'six', 'battle', 'four']\n",
      "there ['it', 'they', 'still', 'said', 'she', 'however', 'now', 'he']\n",
      "will ['would', 'can', 'could', 'may', 'must', 'might', 'should', 'cannot']\n",
      "can ['could', 'would', 'will', 'must', 'may', 'should', 'might', 'cannot']\n",
      "most ['more', 'use', 'many', 'other', 'people', 'both', 'very', 'less']\n",
      "up ['him', 'them', 'back', 'off', 'down', 'out', 'place', 'over']\n",
      "only ['not', 'being', 'usually', 'around', 'possible', 'also', 'still', 'just']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.109:  70%|█████████████████████████████████████                | 140001/200001 [4:49:43<2:43:09,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'had', 'been', 'be', 'became', 'was', 'those']\n",
      "a ['another', 'any', 'the', 'every', 'each', 'very', 'code', 'numbers']\n",
      "i ['you', 'ii', 'we', 'p', 'g', 'f', 'x', 'then']\n",
      "to ['would', 'will', 'not', 'can', 'them', 'could', 'against', 'must']\n",
      "an ['seen', 'dramatically', 'group', 'allowed', 'it', 'shown', 'considered', 'motivations']\n",
      "or ['than', 'and', 'mahajanga', 'alpha', 'riverview', 'though', 'any', 'while']\n",
      "use ['end', 'do', 'way', 'case', 'form', 'example', 'zero', 'death']\n",
      "th ['eight', 'six', 'five', 'seven', 'nine', 'zero', 'four', 'bc']\n",
      "in ['within', 'during', 'under', 'until', 'around', 'among', 'throughout', 'through']\n",
      "united ['nine', 'eight', 'seven', 'five', 'six', 'zero', 'battle', 'four']\n",
      "there ['they', 'it', 'still', 'said', 'she', 'however', 'he', 'now']\n",
      "will ['would', 'could', 'can', 'must', 'might', 'may', 'should', 'cannot']\n",
      "can ['could', 'will', 'must', 'would', 'may', 'should', 'might', 'cannot']\n",
      "most ['more', 'very', 'less', 'use', 'other', 'many', 'some', 'people']\n",
      "up ['him', 'them', 'out', 'off', 'back', 'down', 'place', 'over']\n",
      "only ['usually', 'not', 'also', 'still', 'possible', 'now', 'either', 'around']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.099:  75%|███████████████████████████████████████▋             | 150000/200001 [5:10:39<1:44:13,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'been', 'had', 'became', 'be', 'was', 'being']\n",
      "a ['another', 'any', 'the', 'every', 'each', 'very', 'game', 'numbers']\n",
      "i ['you', 'ii', 'f', 'p', 'we', 'g', 'x', 'k']\n",
      "to ['would', 'will', 'against', 'them', 'must', 'could', 'may', 'can']\n",
      "an ['dramatically', 'group', 'seen', 'shown', 'what', 'allowed', 'it', 'stang']\n",
      "or ['any', 'than', 'and', 'non', 'lepontic', 'certain', 'okina', 'fam']\n",
      "use ['way', 'do', 'form', 'end', 'example', 'case', 'used', 'result']\n",
      "th ['eight', 'six', 'five', 'seven', 'zero', 'nine', 'bc', 'four']\n",
      "in ['within', 'during', 'under', 'around', 'until', 'throughout', 'through', 'between']\n",
      "united ['nine', 'battle', 'six', 'seven', 'zero', 'eight', 'five', 'four']\n",
      "there ['it', 'they', 'still', 'said', 'we', 'now', 'she', 'however']\n",
      "will ['would', 'could', 'can', 'must', 'may', 'might', 'should', 'cannot']\n",
      "can ['could', 'may', 'would', 'must', 'will', 'should', 'might', 'cannot']\n",
      "most ['more', 'very', 'less', 'many', 'use', 'other', 'some', 'people']\n",
      "up ['them', 'out', 'him', 'off', 'down', 'back', 'place', 'over']\n",
      "only ['usually', 'not', 'still', 'possible', 'also', 'just', 'either', 'always']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.105:  80%|██████████████████████████████████████████▍          | 160001/200001 [5:31:37<1:51:39,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'been', 'had', 'became', 'was', 'being', 'be']\n",
      "a ['another', 'any', 'very', 'every', 'the', 'numbers', 'each', 'number']\n",
      "i ['you', 'ii', 'f', 'g', 'p', 'we', 'k', 'x']\n",
      "to ['would', 'will', 'against', 'through', 'must', 'could', 'them', 'should']\n",
      "an ['dramatically', 'seen', 'shown', 'play', 'group', 'ice', 'allowed', 'motivations']\n",
      "or ['and', 'than', 'any', 'non', 'like', 'complex', 'small', 'specific']\n",
      "use ['form', 'way', 'end', 'support', 'do', 'play', 'return', 'case']\n",
      "th ['eight', 'five', 'six', 'nine', 'seven', 'zero', 'bc', 'four']\n",
      "in ['within', 'during', 'under', 'through', 'around', 'until', 'throughout', 'between']\n",
      "united ['nine', 'zero', 'battle', 'seven', 'eight', 'beginning', 'six', 'city']\n",
      "there ['it', 'they', 'still', 'we', 'now', 'he', 'she', 'said']\n",
      "will ['would', 'could', 'can', 'must', 'may', 'might', 'should', 'cannot']\n",
      "can ['could', 'may', 'will', 'would', 'must', 'might', 'should', 'cannot']\n",
      "most ['more', 'less', 'very', 'some', 'many', 'use', 'especially', 'other']\n",
      "up ['out', 'them', 'down', 'him', 'off', 'back', 'over', 'place']\n",
      "only ['usually', 'still', 'not', 'generally', 'either', 'possible', 'just', 'now']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.089:  85%|█████████████████████████████████████████████        | 170001/200001 [5:52:38<1:21:46,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'been', 'had', 'became', 'although', 'being', 'be']\n",
      "a ['another', 'any', 'every', 'each', 'very', 'the', 'numbers', 'relatively']\n",
      "i ['you', 'we', 'ii', 'g', 'f', 'p', 'x', 'god']\n",
      "to ['would', 'will', 'through', 'them', 'against', 'must', 'not', 'should']\n",
      "an ['allowed', 'dramatically', 'seen', 'play', 'shown', 'considered', 'its', 'taken']\n",
      "or ['than', 'and', 'any', 'okina', 'small', 'strong', 'low', 'certain']\n",
      "use ['way', 'form', 'support', 'play', 'view', 'case', 'sense', 'return']\n",
      "th ['eight', 'nine', 'six', 'seven', 'five', 'zero', 'bc', 'four']\n",
      "in ['within', 'during', 'until', 'around', 'under', 'through', 'between', 'since']\n",
      "united ['nine', 'six', 'seven', 'beginning', 'eight', 'battle', 'zero', 'city']\n",
      "there ['they', 'it', 'we', 'still', 'now', 'due', 'she', 'he']\n",
      "will ['would', 'can', 'could', 'must', 'may', 'might', 'should', 'cannot']\n",
      "can ['could', 'will', 'may', 'must', 'would', 'should', 'might', 'cannot']\n",
      "most ['more', 'less', 'many', 'use', 'some', 'very', 'work', 'especially']\n",
      "up ['them', 'out', 'off', 'down', 'back', 'him', 'over', 'place']\n",
      "only ['usually', 'still', 'not', 'always', 'now', 'either', 'generally', 'no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.098:  90%|█████████████████████████████████████████████████▌     | 180001/200001 [6:13:41<56:44,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'had', 'have', 'been', 'became', 'being', 'was', 'be']\n",
      "a ['another', 'any', 'every', 'each', 'numbers', 'the', 'code', 'very']\n",
      "i ['you', 'we', 'ii', 'f', 'g', 'p', 'x', 'k']\n",
      "to ['would', 'will', 'must', 'should', 'through', 'them', 'could', 'against']\n",
      "an ['dramatically', 'cocaine', 'allowed', 'seen', 'shown', 'play', 'taken', 'sentenced']\n",
      "or ['and', 'than', 'any', 'strong', 'low', 'negative', 'alternative', 'certain']\n",
      "use ['support', 'view', 'way', 'form', 'case', 'need', 'sense', 'example']\n",
      "th ['eight', 'nine', 'seven', 'six', 'five', 'zero', 'bc', 'four']\n",
      "in ['within', 'during', 'under', 'until', 'throughout', 'since', 'around', 'through']\n",
      "united ['nine', 'seven', 'eight', 'beginning', 'battle', 'six', 'zero', 'region']\n",
      "there ['it', 'they', 'still', 'now', 'we', 'due', 'he', 'she']\n",
      "will ['would', 'can', 'could', 'must', 'might', 'should', 'cannot', 'may']\n",
      "can ['could', 'must', 'may', 'will', 'would', 'cannot', 'should', 'might']\n",
      "most ['more', 'many', 'use', 'especially', 'some', 'less', 'very', 'work']\n",
      "up ['off', 'out', 'down', 'them', 'back', 'him', 'over', 'place']\n",
      "only ['usually', 'not', 'still', 'always', 'either', 'now', 'no', 'it']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.083:  95%|████████████████████████████████████████████████████▎  | 190001/200001 [6:34:48<27:39,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'had', 'been', 'became', 'being', 'those', 'be']\n",
      "a ['any', 'another', 'every', 'very', 'numbers', 'relatively', 'code', 'number']\n",
      "i ['you', 'we', 'ii', 'f', 'g', 'p', 'x', 'k']\n",
      "to ['will', 'would', 'must', 'not', 'them', 'through', 'could', 'should']\n",
      "an ['cocaine', 'dramatically', 'allowed', 'shown', 'seen', 'stang', 'what', 'self']\n",
      "or ['than', 'any', 'using', 'strong', 'and', 'low', 'another', 'little']\n",
      "use ['example', 'need', 'result', 'way', 'view', 'case', 'sense', 'support']\n",
      "th ['nine', 'eight', 'bc', 'seven', 'five', 'six', 'zero', 'four']\n",
      "in ['within', 'during', 'under', 'until', 'throughout', 'around', 'since', 'through']\n",
      "united ['nine', 'seven', 'six', 'battle', 'zero', 'eight', 'four', 'beginning']\n",
      "there ['they', 'it', 'still', 'he', 'we', 'now', 'she', 'however']\n",
      "will ['would', 'can', 'must', 'could', 'might', 'may', 'should', 'cannot']\n",
      "can ['could', 'must', 'will', 'may', 'cannot', 'would', 'might', 'should']\n",
      "most ['more', 'many', 'less', 'some', 'especially', 'use', 'other', 'very']\n",
      "up ['off', 'out', 'down', 'back', 'them', 'him', 'place', 'over']\n",
      "only ['usually', 'still', 'always', 'now', 'either', 'not', 'generally', 'no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.076: 100%|██████████████████████████████████████████████████████▉| 200000/200001 [6:56:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing closest words\n",
      "were ['are', 'have', 'had', 'been', 'became', 'being', 'those', 'was']\n",
      "a ['another', 'any', 'every', 'very', 'relatively', 'numbers', 'code', 'each']\n",
      "i ['you', 'we', 'ii', 'f', 't', 'g', 'p', 'x']\n",
      "to ['will', 'would', 'must', 'should', 'could', 'may', 'them', 'can']\n",
      "an ['cocaine', 'dramatically', 'shown', 'allowed', 'seen', 'group', 'process', 'what']\n",
      "or ['than', 'and', 'using', 'low', 'any', 'like', 'strong', 'hand']\n",
      "use ['sense', 'support', 'view', 'need', 'way', 'result', 'example', 'practice']\n",
      "th ['eight', 'seven', 'bc', 'nine', 'zero', 'five', 'six', 'nd']\n",
      "in ['within', 'during', 'until', 'under', 'near', 'around', 'throughout', 'since']\n",
      "united ['nine', 'seven', 'beginning', 'eight', 'six', 'battle', 'zero', 'addition']\n",
      "there ['they', 'it', 'still', 'we', 'he', 'she', 'however', 'now']\n",
      "will ['can', 'would', 'could', 'must', 'might', 'should', 'may', 'cannot']\n",
      "can ['will', 'could', 'must', 'may', 'cannot', 'would', 'might', 'should']\n",
      "most ['more', 'many', 'especially', 'less', 'some', 'very', 'particularly', 'other']\n",
      "up ['out', 'off', 'down', 'back', 'them', 'him', 'place', 'over']\n",
      "only ['usually', 'either', 'still', 'now', 'always', 'not', 'even', 'generally']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.076: 100%|███████████████████████████████████████████████████████| 200001/200001 [6:56:00<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a path: ./final_model_1\n"
     ]
    }
   ],
   "source": [
    "run_training(\n",
    "    model_type = 'neg', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 1, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 1, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints_1', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model_1', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VEoba7w3_ps8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file: ./final_model_1/word2vec_neg.model\n",
      "Output file: word_analogy_dev_results_1.txt\n"
     ]
    }
   ],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_model_1', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_dev_results_1.txt', # predicted results\n",
    "    model_type = 'neg' # type of model being used, NLL or NEG\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eyoua1-I_t4A"
   },
   "outputs": [],
   "source": [
    "!chmod 777 evaluate_word_analogy.pl\n",
    "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_1.txt score_neg_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "211TegT0ndEa"
   },
   "source": [
    "<b> Result </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Os5TuVNdth"
   },
   "source": [
    "\n",
    "* Accuracy of Least Illustrative Guesses:            33.2%\n",
    "\n",
    "* Accuracy of Most Illustrative Guesses:             31.7%\n",
    "\n",
    "* Overall Accuracy:                                  32.4%\n",
    "\n",
    "* Takes: 2:23:20 h\n",
    "\n",
    "* Avg loss: 0.065: 100%|██████████| 200001/200001 [2:23:20<00:00, 23.25it/s]\n",
    "\n",
    "Accuracy is less than 33%, lower than expected. Possibly due to the small number of negative samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qljvA7oeNjQJ"
   },
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q84Jr57_NjQK"
   },
   "source": [
    "\n",
    "* Number of Neg samples 3\n",
    "\n",
    "* Learning Rate 1\n",
    "\n",
    "* Window size 3\n",
    "\n",
    "The accuracy of the previous experiment does not exceed 33%, try to increase the accuracy by adding negative samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDlO6nCDNjQK"
   },
   "source": [
    "Expectation: slower than first experiment but more accurat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcO2smirNjQK"
   },
   "outputs": [],
   "source": [
    "run_training(\n",
    "    model_type = 'neg', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 1, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 3, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints_2', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model_2', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGS1UJnY7S3y"
   },
   "outputs": [],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_model_2', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_dev_results_2.txt', # predicted results\n",
    "    model_type = 'neg' # type of model being used, NLL or NEG\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sV0TXbce7fhM"
   },
   "outputs": [],
   "source": [
    "!chmod 777 evaluate_word_analogy.pl\n",
    "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_2.txt score_neg_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7E9-wF5NjQK"
   },
   "source": [
    "<b> Result </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DolvZGynNjQK"
   },
   "source": [
    "\n",
    "\n",
    "* Accuracy of Least Illustrative Guesses: 34.1%\n",
    "\n",
    "* Accuracy of Most Illustrative Guesses: 35.7%\n",
    "\n",
    "* Overall Accuracy: 34.9%\n",
    "\n",
    "* Takes: 2:21:55 h\n",
    "\n",
    "* Avg loss: 0.059: 100%|██████████| 200001/200001 [2:21:55<00:00, 23.49it/s]\n",
    "\n",
    "Accuracy does improve significantly, which is expected since more negative samples allow the model to better distinguish negatively correlated word pairs. But it didn't take longer, which is a bit abnormal, not sure if it is due to Colab's problem with GPU scheduling or other reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBVrNskUNnCl"
   },
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1LDDxfPNnCl"
   },
   "source": [
    "* Number of Neg samples 3\n",
    "\n",
    "* Learning Rate 0.5\n",
    "\n",
    "* Window size 3\n",
    "\n",
    "After the first two experiments, I tried to use a smaller learning rate to explore the impact of the learning rate on the model, and control the negative sample size and window size unchanged\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOkBZCeGNnCm"
   },
   "source": [
    "<b> Expectation </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On5SBOizNnCm"
   },
   "source": [
    "Expecting this experiment to take longer and produce more accurate results than the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDIqINOlNnCm"
   },
   "outputs": [],
   "source": [
    "run_training(\n",
    "    model_type = 'neg', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 0.5, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 3, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints_3', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model_3', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeparMLqA6xz"
   },
   "outputs": [],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_model_3', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_dev_results_3.txt', # predicted results\n",
    "    model_type = 'neg' # type of model being used, NLL or NEG\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgyDSpShDBFa"
   },
   "outputs": [],
   "source": [
    "!chmod 777 evaluate_word_analogy.pl\n",
    "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_3.txt score_neg_3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OODYQp28NnCm"
   },
   "source": [
    "<b> Result </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef-4RoZ4NnCm"
   },
   "source": [
    "\n",
    "* Accuracy of Least Illustrative Guesses: 34.1%\n",
    "\n",
    "* Accuracy of Most Illustrative Guesses: 35.7%\n",
    "\n",
    "* Overall Accuracy: 34.9%\n",
    "\n",
    "* Takes: 5:10:09 h\n",
    "\n",
    "* Avg loss: 0.057: 100%|██████████| 200001/200001 [5:10:09<00:00, 10.75it/s]\n",
    "\n",
    "Surprisingly, the accuracy in all aspects is the same as the previous experiment, and the average loss of the two experiments is also very similar. But still due to Colab's opacity to GPU mobilization, I can't be sure whether the experiment takes longer time is caused by the reduction of learning rate. I infer that the reason why the accuracy has not improved may be that the loss function of the two experiments is also the same because the last experiment has the same variables as the experiment except the learning rate. After a long time of calculation, both experiments have found the lowest point of the function, so no matter how much calculation is done, the result will stay around a value. This makes the learning rate have little effect on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F9QRU4sNvpk"
   },
   "source": [
    "### Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPlyw0cgNvpk"
   },
   "source": [
    "* Number of Neg samples 3\n",
    "\n",
    "* Learning Rate 1\n",
    "\n",
    "* Window size 5\n",
    "\n",
    "After the above experiments, I tried to use a bigger window size to explore the impact of the learning rate on the model, and control the negative sample size and learning rate unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4wAvUweNvpl"
   },
   "source": [
    "Expectation: As the window size becomes larger, the model should sample more related words of a single word, and should be able to more accurately understand the correlation between words and thus give each word a more accurate vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSI_XMtFNvpl"
   },
   "outputs": [],
   "source": [
    "run_training(\n",
    "    model_type = 'neg', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 1, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 3, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints_4', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model_4', # location to save the final model\n",
    "    skip_window = 2, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 4, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJHFmZfqczNz"
   },
   "outputs": [],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_model_4', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_dev_results_4.txt', # predicted results\n",
    "    model_type = 'neg' # type of model being used, NLL or NEG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1lESGSnc1yA"
   },
   "outputs": [],
   "source": [
    "!chmod 777 evaluate_word_analogy.pl\n",
    "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_4.txt score_neg_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faG0h-dpNvpl"
   },
   "source": [
    "<b> Result </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18UrXMUyNvpl"
   },
   "source": [
    "\n",
    "* Accuracy of Least Illustrative Guesses: 34.1%\n",
    "\n",
    "* Accuracy of Most Illustrative Guesses: 35.7%\n",
    "\n",
    "* Overall Accuracy: 34.9%\n",
    "\n",
    "* Takes: 2:21:55 h\n",
    "\n",
    "* Avg loss: 0.058: 100%|██████████| 200001/200001 [2:21:55<00:00, 23.49it/s]\n",
    "\n",
    "The accuracy results are again exactly the same as before. This shocked me. Through repeated confirmation and comparison of avg loss, I made sure that I did not use the same model repeatedly, which made me wonder if there was an error in the implementation. If the implementation is correct, the reason for this result is likely to be the same as the previous experiment - After a long time of calculation, these experiments have found the lowest point of the function, so no matter how much calculation is done, the result will stay near a value. When this value is reached, the literal vector changes very little"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FhkA0iNN0Zx"
   },
   "source": [
    "### Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWNU2PYTN0Zx"
   },
   "source": [
    "* Number of Neg samples 4\n",
    "\n",
    "* Learning Rate 1\n",
    "\n",
    "* Window size 3\n",
    "\n",
    "Since in the second experiment, increasing the number of negative samples significantly improved the accuracy, I tried to further increase the number of negative samples to see if the accuracy would improve further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEOh1RfPN0Zy"
   },
   "source": [
    "Expectation: Looking forward to further improvements in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IJSUIn7N0Zy"
   },
   "outputs": [],
   "source": [
    "run_training(\n",
    "    model_type = 'neg', # defines which loss function is being used to train the model\n",
    "                        # can take values 'nll' for negative log loss and 'neg' for negative sampling\n",
    "    lr = 1, # defines the learning rate used for training the model\n",
    "    num_neg_samples_per_center = 4, # controls the number of negative samples per center word\n",
    "    checkpoint_model_path = './checkpoints_5', # defines path to the checkpoint of the model\n",
    "    final_model_path = './final_model_5', # location to save the final model\n",
    "    skip_window = 1, # size of the skip window\n",
    "    vocab_size = int(1e5), # size of the vocabulary used in the experiments\n",
    "    num_skips = 2, # Number of samples to be drawn from a window\n",
    "    batch_size = 64, # Size of the batches in terms of number of x,y pairs used for training\n",
    "    embedding_size = 128, # size of the embedding vectores\n",
    "    checkpoint_step = 50000, # Number of steps after which checkpoint is saved\n",
    "    max_num_steps = 200001 # Maximum number of steps to train for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wp_wbS59oYgA"
   },
   "outputs": [],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_model_5', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_dev_results_5.txt', # predicted results\n",
    "    model_type = 'neg' # type of model being used, NLL or NEG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7aumNqWpRY5"
   },
   "outputs": [],
   "source": [
    "!chmod 777 evaluate_word_analogy.pl\n",
    "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_5.txt score_neg_5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc7WuBtTN0Zy"
   },
   "source": [
    "<b> Result </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwISJiMiN0Zz"
   },
   "source": [
    "* Accuracy of Least Illustrative Guesses: 29.5%\n",
    "\n",
    "* Accuracy of Most Illustrative Guesses: 33.5%\n",
    "\n",
    "* Overall Accuracy: 31.5%\n",
    "\n",
    "* Takes: 2:22:35 h\n",
    "\n",
    "* Avg loss: 0.059: 100%|██████████| 200001/200001 [2:22:35<00:00, 23.38it/s]\n",
    "\n",
    "Accuracy did not improve, but decreased. Maybe too many negative samples interfere with the model's judgment on the relationship between words, making many related word pairs less relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y_UyAKpdY7C"
   },
   "source": [
    "# Running the Negative Log-likelihood (NLL) method.\n",
    "\n",
    "Learn word vectors using the negative log-likelihood method with the same settings of hyper parameters as in Experiment 1 above. (Note that number of negative samples does not apply in this case). Test the resulting vectors on the test set of the word analogy task.\n",
    "<br/>\n",
    "\n",
    "\n",
    "<b>Result</b>\n",
    "\n",
    "* Accuracy of Least Illustrative Guesses: 26.0%\n",
    "* Accuracy of Most Illustrative Guesses: 30.4%\n",
    "* Accuracy of Most Illustrative Guesses: 30.4%\n",
    "* Time: 0:30:56 h\n",
    "\n",
    "The nll model has lower accuracy with the same settings as the neg model, but it takes much less time than the neg model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PgFzXEfritZ"
   },
   "outputs": [],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_model', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_dev.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'word_analogy_dev_results_nll.txt', # predicted results\n",
    "    model_type = 'nll' # type of model being used, NLL or NEG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jalBbGeerqGm"
   },
   "outputs": [],
   "source": [
    "!chmod 777 evaluate_word_analogy.pl\n",
    "!./evaluate_word_analogy.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_results_nll.txt score_nll.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5McV-znN9K1"
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWOjjGMsN746"
   },
   "source": [
    "|EXPERIMENT #| Accuracy | Time | Avg loss |\n",
    "| ----------- | ----------- |----------|------|\n",
    "| NEG 1      | 32.4%       | 2:23:20 | 0.065|\n",
    "| NEG 2   | 34.9%       |2:21:55|0.059|\n",
    "|NEG 3| 34.9%| 5:10:09| 0.057|\n",
    "|NEG 4| 34.9%| 2:21:55| 0.058|\n",
    "|NEG 5| 31.5%| 2:22:35|0.059|\n",
    "|NLL 1| 28.2%|0:30:56| 1.091|\n",
    "\n",
    "In general, through the above experiments, it can be found that a smaller learning rate can find the minimum value of the loss function more accurately, but it has no significant effect after being small to a certain extent. It is also found that more negative samples can improve the accuracy of learning, but once the number of negative samples is too large, it will also interfere with the model. The change of the window size should have a similar performance to the negative sample, but due to the small number of experiments on the window size, it is not obvious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxBHHqj_Oz_s"
   },
   "source": [
    "## WEAT Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbPr6A28O2FO"
   },
   "source": [
    "Observe the unwanted learnings of the generated embeddings.\n",
    "In this task, we looked at how to evaluate whether the embeddings are biased or not.\n",
    "\n",
    "The WEAT test provides a way to measure quantifiably the bias in the word embeddings. [This paper](https://arxiv.org/pdf/1810.03611.pdf) describes the method in detail.\n",
    "\n",
    "The basic idea is to examine the associations in word embeddings between concepts.\n",
    "It measures the degree to which a model associates sets of target words (e.g., African American names, European American names, flowers, insects) with sets of attribute words (e.g., ”stable”, ”pleasant” or ”unpleasant”).\n",
    "The association between two given words is defined as the cosine similarity between the embedding vectors for the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egvyoVBmUyeO"
   },
   "source": [
    "This will generate the bias scores as evaluated on 5 different tasks with different sets of attributes (A and B) and targets (X and Y) as defined in the file pointed to in the `weat_file_path` (`weat.json` for the given data). This will print and dump the output in the filepath.\n",
    "\n",
    "\n",
    "Add to the json file `custom_weat.json`, another task in the following format:\n",
    "```\n",
    "{\n",
    "  # initial tasks....\n",
    "  \"custom_task\": {\n",
    "    \"A_key\": \"A_val\",\n",
    "    \"B_key\": \"B_val\",\n",
    "    \"X_key\": \"X_val\",\n",
    "    \"Y_key\": \"Y_val\",\n",
    "    \"A_val\": [\n",
    "      # list of words for attribute A\n",
    "    ],\n",
    "    \"B_val\": [\n",
    "      # list of words for attribute B\n",
    "    ],\n",
    "    \"X_val\": [\n",
    "      # list of words for target X\n",
    "    ],\n",
    "    \"Y_val\": [\n",
    "      # list of words for target Y\n",
    "    ],\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Ensure that the task name is `custom_task`, this will be automatically verified. Have a look at the other tasks for more clarity.\n",
    "\n",
    "Your submission bias output files should be named `nll_bias_output.json` and `neg_bias_output.json`.\n",
    "\n",
    "After you complete the `custom_weat.json` task, you can run the script for the given data as well as your custom data.\n",
    "Your submission custom bias output files should be named `nll_custom_bias_output.json` and `neg_custom_bias_output.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AscW6lztS2Vn"
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "       return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "\n",
    "def unit_vector(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "\n",
    "    \"\"\"\n",
    "    Cosine Similarity between the 2 vectors\n",
    "    \"\"\"\n",
    "\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.clip(np.tensordot(v1_u, v2_u, axes=(-1, -1)), -1.0, 1.0)\n",
    "\n",
    "def weat_association(W, A, B):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute Weat score for given target words W, along the attributes A & B.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(cos_sim(W, A), axis=-1) - np.mean(cos_sim(W, B), axis=-1)\n",
    "\n",
    "def weat_score(X, Y, A, B):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute differential weat score across the given target words X & Y along the attributes A & B.\n",
    "    \"\"\"\n",
    "\n",
    "    x_association = weat_association(X, A, B)\n",
    "    y_association = weat_association(Y, A, B)\n",
    "\n",
    "    tmp1 = np.mean(x_association, axis=-1) - np.mean(y_association, axis=-1)\n",
    "    tmp2 = np.std(np.concatenate((x_association, y_association), axis=0))\n",
    "\n",
    "    return tmp1 / tmp2\n",
    "\n",
    "def balance_word_vectors(vec1, vec2):\n",
    "    diff = len(vec1) - len(vec2)\n",
    "\n",
    "    if diff > 0:\n",
    "        vec1 = np.delete(vec1, np.random.choice(len(vec1), diff, 0), axis=0)\n",
    "    else:\n",
    "        vec2 = np.delete(vec2, np.random.choice(len(vec2), -diff, 0), axis=0)\n",
    "\n",
    "    return (vec1, vec2)\n",
    "\n",
    "def get_word_vectors(words, model, vocab_token_to_id):\n",
    "\n",
    "    \"\"\"\n",
    "    Return list of word embeddings for the given words using the passed model and tokeniser\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    emb_size = len(model[0])\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            output.append(model[vocab_token_to_id[word]])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.array(output)\n",
    "\n",
    "def compute_weat(weat_path, model, vocab_token_to_id):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute WEAT score for the task as defined in the file at `weat_path`, and generating word embeddings from the passed model and tokeniser.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(weat_path) as f:\n",
    "        weat_dict = json.load(f)\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for data_name, data_dict in weat_dict.items():\n",
    "        # Target\n",
    "        X_key = data_dict['X_key']\n",
    "        Y_key = data_dict['Y_key']\n",
    "\n",
    "        # Attributes\n",
    "        A_key = data_dict['A_key']\n",
    "        B_key = data_dict['B_key']\n",
    "\n",
    "        X = get_word_vectors(data_dict[X_key], model, vocab_token_to_id)\n",
    "        Y = get_word_vectors(data_dict[Y_key], model, vocab_token_to_id)\n",
    "        A = get_word_vectors(data_dict[A_key], model, vocab_token_to_id)\n",
    "        B = get_word_vectors(data_dict[B_key], model, vocab_token_to_id)\n",
    "\n",
    "        if len(X) == 0 or len(Y) == 0:\n",
    "            print('Not enough matching words in dictionary')\n",
    "            continue\n",
    "\n",
    "        X, Y = balance_word_vectors(X, Y)\n",
    "        A, B = balance_word_vectors(A, B)\n",
    "\n",
    "        score = weat_score(X, Y, A, B)\n",
    "        all_scores[data_name] = str(score)\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "def dump_dict(obj, output_path):\n",
    "    with open(output_path, \"w\") as file:\n",
    "        json.dump(obj, file)\n",
    "\n",
    "def run_bias_eval(\n",
    "    weat_file_path = 'weat.json', # weat file where the tasks are defined\n",
    "    out_file = 'weat_demo_results.json', # output JSON file where the output is stored\n",
    "    model_path = '/content/final_demo_model/word2vec_nll.model' # Full model path (including filename) to load from\n",
    "):\n",
    "\n",
    "    vocab_token_to_id, model = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    bias_score = compute_weat(weat_file_path, model, vocab_token_to_id)\n",
    "\n",
    "    print(\"Final Bias Scores\")\n",
    "    print(json.dumps(bias_score, indent=4))\n",
    "\n",
    "    dump_dict(bias_score, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNDgCfEoV7Bz"
   },
   "outputs": [],
   "source": [
    "run_bias_eval(\n",
    "    weat_file_path = 'weat.json', # weat file where the tasks are defined\n",
    "    out_file = 'nll_bias_output.json', # output JSON file where the output is stored\n",
    "    model_path = '/content/final_model/word2vec_nll.model' # Full model path (including filename) to load from\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs34E6aREq_a"
   },
   "source": [
    "Please refer weat.json as show in the above code and create 5 new tests for your best NLL and NEG models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPudRwGkFW_D"
   },
   "source": [
    "### WEAT Experiment 1 (NLL Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfRon5nyFW_F"
   },
   "source": [
    "<b> What tests did you create and why do you expect these biases to exist in the model?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ta_TKYoFW_F"
   },
   "source": [
    "Tests Created:\n",
    "* AmusementPark_Hospital_Pleasant_Unpleasant\n",
    "\n",
    "* ItalianCuisines_MexicanCuisines_Healthy_Unhealthy\n",
    "\n",
    "* JapeneseCar_AmericanCar_Good_Bad\n",
    "\n",
    "* EuropeanCountries_AfricanCountries_Developed_FallBehind\n",
    "\n",
    "* Male_Female_Careless_Careful\n",
    "\n",
    "I created these tests because these biases in testing are very common in many people's minds, so there is a good chance that this bias will be reflected in the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e04p30ZjFW_F"
   },
   "source": [
    "<b> How do you expect the model to behave? What is the expected score in your opinion? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSjFSkewFW_F"
   },
   "source": [
    "I expect that for AmusementPark_Hospital_Pleasant_Unpleasant test and EuropeanCountries_AfricanCountries_Developed_FallBehind test, the model can give a higher correlation, because these prejudices are more common and deep-rooted in daily life, I expect their results to be greater than other tests, and other results I generally Expect to be higher than 0, but shouldn't be much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1Aim3mtFW_F"
   },
   "outputs": [],
   "source": [
    "run_bias_eval(\n",
    "    weat_file_path = 'custom_weat.json', # weat file where the tasks are defined\n",
    "    out_file = 'weat_results_nll.json', # output JSON file where the output is stored\n",
    "    model_path = '/content/final_model/word2vec_nll.model' # Full model path (including filename) to load from\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYU-gezgFW_F"
   },
   "source": [
    "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCg5ozY2FW_F"
   },
   "source": [
    "* \"AmusementPark_Hospital_Pleasant_Unpleasant\": \"1.3171837\"\n",
    "* \"ItalianCuisines_MexicanCuisines_Healthy_Unhealthy\": \"0.42497307\"\n",
    "* \"JapeneseCar_AmericanCar_Good_Bad\": \"-0.20283286\",\n",
    "* \"EuropeanCountries_AfricanCountries_Developed_FallBehind\": \"0.12414764\"\n",
    "* \"Male_Female_Careless_Careful\": \"1.2796223\"\n",
    "\n",
    "The results of AmusementPark_Hospital_Pleasant_Unpleasant are as expected, amusement parks are usually pleasant and hospitals are usually accompanied by illness and pain. The test results of ItalianCuisines_MexicanCuisines_Healthy_Unhealthy are also more in line with expectations. But the results of JapeneseCar_AmericanCar_Good_Bad are opposite to expectations, which means that the model is more inclined to think that American cars are better. For the results given by EuropeanCountries_AfricanCountries_Developed_FallBehind, it can be found that the bias of this test is not so large. In the end, I was surprised by the bias in the model regarding the relationship between gender and personality. Model thinks that Male are more closely associated with Careless and Female are more closely associated with Careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAfs_tRbGIRq"
   },
   "source": [
    "<b> Please suggest 2 possible ways to remove bias and why do you think they will work? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGKSMoA0rq2K"
   },
   "source": [
    "* I think one way is to ensure that the data used to train the algorithm is diverse and representative of the population. Data should be collected from different sources, including different age groups, genders, ethnicities, and socioeconomic backgrounds. By having a diverse dataset, the model can learn to recognize patterns and relationships that are not biased towards a particular group.\n",
    "\n",
    "* I think another way is to monitor some of the more common biases in real time during the training sample process. Once the biases are found to be serious, start looking for sample inputs with opposite biases to correct them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igvl7TWEF-V1"
   },
   "source": [
    "### WEAT Experiment 2 (NEG Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4Ba0eWbF-V1"
   },
   "source": [
    "<b> What tests did you create and why do you expect these biases to exist in the model?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2jAWsGHF-V1"
   },
   "source": [
    "Tests Created:\n",
    "* AmusementPark_Hospital_Pleasant_Unpleasant\n",
    "\n",
    "* ItalianCuisines_MexicanCuisines_Healthy_Unhealthy\n",
    "\n",
    "* JapeneseCar_AmericanCar_Good_Bad\n",
    "\n",
    "* EuropeanCountries_AfricanCountries_Developed_FallBehind\n",
    "\n",
    "* Male_Female_Careless_Careful\n",
    "\n",
    "I created these tests because these biases in testing are very common in many people's minds, so there is a good chance that this bias will be reflected in the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fx3MD81F-V1"
   },
   "source": [
    "<b> How do you expect the model to behave? What is the expected score in your opinion? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oL2uHz1F-V2"
   },
   "source": [
    "I was expecting similar results from this experiment as the previous ones, since they were trained with the same dataset. Perhaps the results of this experiment will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_GmPk02F-V2"
   },
   "outputs": [],
   "source": [
    "run_bias_eval(\n",
    "    weat_file_path = 'custom_weat.json', # weat file where the tasks are defined\n",
    "    out_file = 'weat_results_neg.json', # output JSON file where the output is stored\n",
    "    model_path = '/content/final_model/word2vec_neg.model' # Full model path (including filename) to load from\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXmYJsdAF-V3"
   },
   "source": [
    "<b> What did you observe? Do the observations follow your expectations? Give a plausible cause.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpuoBFU7F-V4"
   },
   "source": [
    "* \"AmusementPark_Hospital_Pleasant_Unpleasant\": \"1.4622178\",\n",
    "* \"ItalianCuisines_MexicanCuisines_Healthy_Unhealthy\": \"0.82199323\",\n",
    "* \"JapeneseCar_AmericanCar_Good_Bad\": \"-0.23306134\",\n",
    "* \"EuropeanCountries_AfricanCountries_Developed_FallBehind\": \"0.060268328\",\n",
    "* \"Male_Female_Careless_Careful\": \"-0.4530505\"\n",
    "\n",
    "Except for the results of the Male_Female_Careless_Carefu test, which are quite different from the previous ones, the results of other tests are relatively similar. The difference in the \"Male_Female_Careless_Careful\" test may be due to the fact that too many male and careless samples were drawn when negative samples were drawn, which corrected the original bias and even made the bias develop in the opposite direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNaaE96eGkIQ"
   },
   "source": [
    "<b> Please suggest 2 possible ways to remove bias and why do you think they will work? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuCpF_R1GkIQ"
   },
   "source": [
    "* One way is as before: ensure that the data used to train the algorithm is diverse and representative of the population. Data should be collected from different sources, including different age groups, genders, ethnicities, and socioeconomic backgrounds. By having a diverse dataset, the model can learn to recognize patterns and relationships that are not biased towards a particular group.\n",
    "\n",
    "* Another possible method is to take some biased samples at the same time when drawing negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVB9U2zZSHX4"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP-45LCiIKhR"
   },
   "source": [
    "<b> Please provide an appropriate conclusion to the experiments and results you obtained </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLOkRdEnIKhR"
   },
   "source": [
    "Through the above experiments, the following conclusions can be obtained. neg is often more accurate than nll when using the same hyper-parameters. In addition, for the neg model, the appropriate window size and the number of negative samples can make the model more accurate, too much or too little will interfere with the accuracy. A small learning rate can find the minimum point more accurately. In addition, many biases that are common in real life can be reflected in the model. Whether these biases are measured by the nll or neg model, as long as the samples are the same, the results are relatively close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT5Sc4qKIaeb"
   },
   "source": [
    "## Submission Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anunXVRPIttD"
   },
   "source": [
    "Create a folder having your solution and should contain the following:\n",
    "  - This notebook with your solution\n",
    "  - All linked files provided in the 'Files to upload in notebook' folder\n",
    "  - A 'solution/' folder containing the files mentioned below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZ-ZZi8dJIfF"
   },
   "source": [
    "Files to be generated and submitted:\n",
    "Create a new folder called `submission/` and place the following files in it:\n",
    "   - `test_preds_nll.txt` - Your best NLL model predictions for `word_analogy_test.txt`\n",
    "   - `test_preds_neg.txt` - Your best negative sampling model predictions for `word_analogy_test.txt`\n",
    "   - `nll_bias_output.json` - Results for the WEAT task on `weat.json` using your best NLL model\n",
    "   - `neg_bias_output.json` - Results for the WEAT task on `weat.json` using your best negative sampling model\n",
    "   - `nll_custom_bias_output.json` - Results for the custom WEAT task on `custom_weat.json` using your best NLL model\n",
    "   - `neg_custom_bias_output.json` - Results for the custom WEAT task on `custom_weat.json` using your best negative sampling model\n",
    "   - `gdrive_link.txt` - Should contain a `wget`able to a folder that contains your best models. The model files should be named `word2vec_nll.model` and `word2vec_neg.model`, and the folder should be named `538-hw1-<SBUID>-models`. Please make sure you provide the necessary permissions.\n",
    "   - `<SBUID>_Report.pdf` - A PDF report as detailed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwW7fXswekgd"
   },
   "outputs": [],
   "source": [
    "run_word_analogy_eval(\n",
    "    model_path = './final_model', # path to the location where the model being evaluated is stored\n",
    "    input_filepath = 'word_analogy_test.txt', # Word analogy file to evaluate on\n",
    "    output_filepath = 'test_preds_nll.txt', # predicted results\n",
    "    model_type = 'nll' # type of model being used, NLL or NEG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyPpY2YrJsHK"
   },
   "source": [
    "## Collaboration Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnnFuq1xLW_F"
   },
   "source": [
    "  - You can collaborate to discuss ideas and to help each other for better understanding of concepts and math.\n",
    "  - You should NOT collaborate on the code level. This includes all implementation activities: design, coding, and debugging.\n",
    "  - You should NOT not use any code that you did not write to complete the assignment.\n",
    "  - The homework will be **cross-checked**. Do not cheat at all! It’s worth doing the homework partially instead of cheating and copying your code and get 0 for the whole homework. In previous years, students have faced harsh disciplinary action as a result of the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVMJ3Q3NLbgb"
   },
   "source": [
    "## Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqv2Rxr2MM5k"
   },
   "source": [
    "  - If you add any code apart from the TODOs in the codebase (note that you don't need to), please mark it by commenting in the code itself.\n",
    "  An example of the same could be:\n",
    "    ```\n",
    "    # Adding some_global_var for XXX\n",
    "    some_global_var\n",
    "    # NAME MODIFIED FOR BETTER EXPERIMENT EXPERIENCE\n",
    "    def read_data_analogy(file_path)\n",
    "    ```\n",
    "  - General tips when you work on tensor computations:\n",
    "    - Break the whole list of operations into smaller ones.\n",
    "    - Write down the shapes of the tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFM1yE54MOE6"
   },
   "source": [
    "## Credits and Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnWdapCZMXXf"
   },
   "source": [
    "**Credits**: This code is part of the starter package of the assignment/s used in NLP course at Stony Brook University.\n",
    "This assignment has been designed, implemented and revamped as required by many NLP TAs to varying degrees.\n",
    "In chronological order of TAship they include Heeyoung Kwon, Jun Kang, Mohaddeseh Bastan, Harsh Trivedi, Matthew Matero, Nikita Soni, Sharvil Katariya, Yash Kumar Lal, Adithya V. Ganesan, Sounak Mondal, Saqib Hasan, and Jasdeep Grover. Thanks to all of them!\n",
    "\n",
    "**Disclaimer/License**: This code is only for school assignment purpose, and **any version of this should NOT be shared publicly on github or otherwise even after semester ends**.\n",
    "Public availability of answers devalues usability of the assignment and work of several TAs who have contributed to this.\n",
    "We hope you'll respect this restriction."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7m0Q604fnADQ"
   ],
   "gpuClass": "premium",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
